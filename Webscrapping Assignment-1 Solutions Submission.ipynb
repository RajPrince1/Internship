{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c7fcab4",
   "metadata": {},
   "source": [
    "# -------------------------------WEBSCRAPPING ASSIGNMENT-1--------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b5b1350",
   "metadata": {},
   "source": [
    "                                                                             Submission Date : 1st September 2022"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d462a21",
   "metadata": {},
   "source": [
    "# Q1.Python program to display all the header tags from wikipedia.org\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "493c0e4c",
   "metadata": {},
   "source": [
    "# Scrapping as follows :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "18d5e3ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['h1 Main Page',\n",
       " 'h1 Welcome to Wikipedia',\n",
       " \"h2 From today's featured article\",\n",
       " 'h2 Did you know\\xa0...',\n",
       " 'h2 In the news',\n",
       " 'h2 On this day',\n",
       " \"h2 Today's featured picture\",\n",
       " 'h2 Other areas of Wikipedia',\n",
       " \"h2 Wikipedia's sister projects\",\n",
       " 'h2 Wikipedia languages',\n",
       " 'h2 Navigation menu',\n",
       " 'h3 Personal tools',\n",
       " 'h3 Namespaces',\n",
       " 'h3 Views',\n",
       " 'h3 Search',\n",
       " 'h3 Navigation',\n",
       " 'h3 Contribute',\n",
       " 'h3 Tools',\n",
       " 'h3 Print/export',\n",
       " 'h3 In other projects',\n",
       " 'h3 Languages']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from  bs4 import BeautifulSoup\n",
    "import  requests\n",
    "\n",
    "url = \"https://en.wikipedia.org/wiki/Main_Page\"\n",
    "\n",
    "page = requests.get(url)\n",
    "page\n",
    "content=BeautifulSoup(page.content)\n",
    "content\n",
    "\n",
    "Header = []\n",
    "\n",
    "for header in content.find_all([\"h1\", \"h2\", \"h3\", \"h4\",\"h5\", \"h6\"]):\n",
    "    Header.append(header.name+\" \"+header.text.strip())\n",
    "Header    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17553a18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c822ee7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "19642f47",
   "metadata": {},
   "source": [
    "# Q2.Python program to display IMDB’s Top rated 100 movies’ data\n",
    "\n",
    "Scrapping Name, Rating, Year of Release and its data frame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bdda11d",
   "metadata": {},
   "source": [
    "# Scrapping as follows :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "55738b6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in c:\\users\\clickmoney\\anaconda3\\lib\\site-packages (2.26.0)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\clickmoney\\anaconda3\\lib\\site-packages (from requests) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\clickmoney\\anaconda3\\lib\\site-packages (from requests) (1.26.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\clickmoney\\anaconda3\\lib\\site-packages (from requests) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\clickmoney\\anaconda3\\lib\\site-packages (from requests) (3.2)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\clickmoney\\anaconda3\\lib\\site-packages (4.10.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\clickmoney\\anaconda3\\lib\\site-packages (from beautifulsoup4) (2.2.1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Year</th>\n",
       "      <th>Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The Shawshank Redemption</td>\n",
       "      <td>1994)</td>\n",
       "      <td>9.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The Godfather</td>\n",
       "      <td>1972)</td>\n",
       "      <td>9.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The Dark Knight</td>\n",
       "      <td>2008)</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The Lord of the Rings: The Return of the King</td>\n",
       "      <td>2003)</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Schindler's List</td>\n",
       "      <td>1993)</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>The Godfather Part II</td>\n",
       "      <td>1974)</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>12 Angry Men</td>\n",
       "      <td>1957)</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Pulp Fiction</td>\n",
       "      <td>1994)</td>\n",
       "      <td>8.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Inception</td>\n",
       "      <td>2010)</td>\n",
       "      <td>8.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>The Lord of the Rings: The Two Towers</td>\n",
       "      <td>2002)</td>\n",
       "      <td>8.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Fight Club</td>\n",
       "      <td>1999)</td>\n",
       "      <td>8.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>The Lord of the Rings: The Fellowship of the Ring</td>\n",
       "      <td>2001)</td>\n",
       "      <td>8.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Forrest Gump</td>\n",
       "      <td>1994)</td>\n",
       "      <td>8.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Il buono, il brutto, il cattivo</td>\n",
       "      <td>1966)</td>\n",
       "      <td>8.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>The Matrix</td>\n",
       "      <td>1999)</td>\n",
       "      <td>8.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Goodfellas</td>\n",
       "      <td>1990)</td>\n",
       "      <td>8.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>The Empire Strikes Back</td>\n",
       "      <td>1980)</td>\n",
       "      <td>8.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>One Flew Over the Cuckoo's Nest</td>\n",
       "      <td>1975)</td>\n",
       "      <td>8.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Interstellar</td>\n",
       "      <td>2014)</td>\n",
       "      <td>8.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Cidade de Deus</td>\n",
       "      <td>2002)</td>\n",
       "      <td>8.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Sen to Chihiro no kamikakushi</td>\n",
       "      <td>2001)</td>\n",
       "      <td>8.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Saving Private Ryan</td>\n",
       "      <td>1998)</td>\n",
       "      <td>8.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>The Green Mile</td>\n",
       "      <td>1999)</td>\n",
       "      <td>8.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>La vita è bella</td>\n",
       "      <td>1997)</td>\n",
       "      <td>8.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Se7en</td>\n",
       "      <td>1995)</td>\n",
       "      <td>8.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Terminator 2: Judgment Day</td>\n",
       "      <td>1991)</td>\n",
       "      <td>8.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>The Silence of the Lambs</td>\n",
       "      <td>1991)</td>\n",
       "      <td>8.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Star Wars</td>\n",
       "      <td>1977)</td>\n",
       "      <td>8.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Seppuku</td>\n",
       "      <td>1962)</td>\n",
       "      <td>8.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Shichinin no samurai</td>\n",
       "      <td>1954)</td>\n",
       "      <td>8.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>It's a Wonderful Life</td>\n",
       "      <td>1946)</td>\n",
       "      <td>8.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Gisaengchung</td>\n",
       "      <td>2019)</td>\n",
       "      <td>8.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Whiplash</td>\n",
       "      <td>2014)</td>\n",
       "      <td>8.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Top Gun: Maverick</td>\n",
       "      <td>2022)</td>\n",
       "      <td>8.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>The Intouchables</td>\n",
       "      <td>2011)</td>\n",
       "      <td>8.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>The Prestige</td>\n",
       "      <td>2006)</td>\n",
       "      <td>8.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>The Departed</td>\n",
       "      <td>2006)</td>\n",
       "      <td>8.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>The Pianist</td>\n",
       "      <td>2002)</td>\n",
       "      <td>8.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>Gladiator</td>\n",
       "      <td>2000)</td>\n",
       "      <td>8.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>American History X</td>\n",
       "      <td>1998)</td>\n",
       "      <td>8.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>The Usual Suspects</td>\n",
       "      <td>1995)</td>\n",
       "      <td>8.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>Léon</td>\n",
       "      <td>1994)</td>\n",
       "      <td>8.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>The Lion King</td>\n",
       "      <td>1994)</td>\n",
       "      <td>8.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>Nuovo Cinema Paradiso</td>\n",
       "      <td>1988)</td>\n",
       "      <td>8.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>Hotaru no haka</td>\n",
       "      <td>1988)</td>\n",
       "      <td>8.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>Back to the Future</td>\n",
       "      <td>1985)</td>\n",
       "      <td>8.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>Apocalypse Now</td>\n",
       "      <td>1979)</td>\n",
       "      <td>8.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>Alien</td>\n",
       "      <td>1979)</td>\n",
       "      <td>8.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>Once Upon a Time in the West</td>\n",
       "      <td>1968)</td>\n",
       "      <td>8.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>Psycho</td>\n",
       "      <td>1960)</td>\n",
       "      <td>8.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Title   Year Rating\n",
       "0                            The Shawshank Redemption  1994)    9.3\n",
       "1                                       The Godfather  1972)    9.2\n",
       "2                                     The Dark Knight  2008)    9.0\n",
       "3       The Lord of the Rings: The Return of the King  2003)    9.0\n",
       "4                                    Schindler's List  1993)    9.0\n",
       "5                               The Godfather Part II  1974)    9.0\n",
       "6                                        12 Angry Men  1957)    9.0\n",
       "7                                        Pulp Fiction  1994)    8.9\n",
       "8                                           Inception  2010)    8.8\n",
       "9               The Lord of the Rings: The Two Towers  2002)    8.8\n",
       "10                                         Fight Club  1999)    8.8\n",
       "11  The Lord of the Rings: The Fellowship of the Ring  2001)    8.8\n",
       "12                                       Forrest Gump  1994)    8.8\n",
       "13                    Il buono, il brutto, il cattivo  1966)    8.8\n",
       "14                                         The Matrix  1999)    8.7\n",
       "15                                         Goodfellas  1990)    8.7\n",
       "16                            The Empire Strikes Back  1980)    8.7\n",
       "17                    One Flew Over the Cuckoo's Nest  1975)    8.7\n",
       "18                                       Interstellar  2014)    8.6\n",
       "19                                     Cidade de Deus  2002)    8.6\n",
       "20                      Sen to Chihiro no kamikakushi  2001)    8.6\n",
       "21                                Saving Private Ryan  1998)    8.6\n",
       "22                                     The Green Mile  1999)    8.6\n",
       "23                                    La vita è bella  1997)    8.6\n",
       "24                                              Se7en  1995)    8.6\n",
       "25                         Terminator 2: Judgment Day  1991)    8.6\n",
       "26                           The Silence of the Lambs  1991)    8.6\n",
       "27                                          Star Wars  1977)    8.6\n",
       "28                                            Seppuku  1962)    8.6\n",
       "29                               Shichinin no samurai  1954)    8.6\n",
       "30                              It's a Wonderful Life  1946)    8.6\n",
       "31                                       Gisaengchung  2019)    8.5\n",
       "32                                           Whiplash  2014)    8.5\n",
       "33                                  Top Gun: Maverick  2022)    8.5\n",
       "34                                   The Intouchables  2011)    8.5\n",
       "35                                       The Prestige  2006)    8.5\n",
       "36                                       The Departed  2006)    8.5\n",
       "37                                        The Pianist  2002)    8.5\n",
       "38                                          Gladiator  2000)    8.5\n",
       "39                                 American History X  1998)    8.5\n",
       "40                                 The Usual Suspects  1995)    8.5\n",
       "41                                               Léon  1994)    8.5\n",
       "42                                      The Lion King  1994)    8.5\n",
       "43                              Nuovo Cinema Paradiso  1988)    8.5\n",
       "44                                     Hotaru no haka  1988)    8.5\n",
       "45                                 Back to the Future  1985)    8.5\n",
       "46                                     Apocalypse Now  1979)    8.5\n",
       "47                                              Alien  1979)    8.5\n",
       "48                       Once Upon a Time in the West  1968)    8.5\n",
       "49                                             Psycho  1960)    8.5"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip install requests\n",
    "import requests\n",
    "\n",
    "topics_url = 'https://www.imdb.com/search/title/?groups=top_100&sort=user_rating,desc'\n",
    "response = requests.get(topics_url)\n",
    "\n",
    "page_content = response.text\n",
    "page_content\n",
    "\n",
    "!pip install beautifulsoup4\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "doc = BeautifulSoup(response.text, 'html.parser')\n",
    "doc.find('title')\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def get_topics_page():\n",
    "    topic_url = 'https://www.imdb.com/search/title/?groups=top_100&sort=user_rating,desc'\n",
    "    response=requests.get(topic_url)\n",
    "    \n",
    "    if response.status_code != 200:\n",
    "        raise Exception(f'Failed to load page {topic_url}')\n",
    "    \n",
    "    doc = BeautifulSoup(response.text, 'html.parser')\n",
    "    return doc\n",
    "\n",
    "doc=get_topics_page()\n",
    "\n",
    "def get_movie_titles(doc):\n",
    "    \n",
    "    selection_class=\"lister-item-header\"\n",
    "    movie_title_tags=doc.find_all('h3',{'class':selection_class})\n",
    "    movie_titles=[]\n",
    "\n",
    "    for tag in movie_title_tags:\n",
    "        title = tag.find('a').text\n",
    "        movie_titles.append(title)\n",
    "          \n",
    "    return movie_titles\n",
    "titles = get_movie_titles(doc)\n",
    "\n",
    "titles[:100]\n",
    "\n",
    "def get_movie_year(doc):\n",
    "    year_selector = \"lister-item-year text-muted unbold\"           \n",
    "    movie_year_tags=doc.find_all('span',{'class':year_selector})\n",
    "    movie_year_tagss=[]\n",
    "    for tag in movie_year_tags:\n",
    "        movie_year_tagss.append(tag.get_text().strip()[1:100])\n",
    "    return movie_year_tagss\n",
    "\n",
    "years = get_movie_year(doc)\n",
    "years[:100]\n",
    "\n",
    "def get_movie_rating(doc):\n",
    "    rating_selector=\"inline-block ratings-imdb-rating\"            \n",
    "    movie_rating_tags=doc.find_all('div',{'class':rating_selector})\n",
    "    movie_rating_tagss=[]\n",
    "    for tag in movie_rating_tags:\n",
    "        movie_rating_tagss.append(tag.get_text().strip())\n",
    "    return movie_rating_tagss\n",
    "\n",
    "ratings = get_movie_rating(doc)\n",
    "ratings[:100]\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "def all_pages():\n",
    "\n",
    "    movies_dict={'title':[],'year':[],'rating':[]}\n",
    "  \n",
    "    for i in range(1,2000,100):\n",
    "       \n",
    "        try:\n",
    "            url = 'https://www.imdb.com/search/title/?groups=top_100&sort=user_rating,desc'\n",
    "            response = requests.get(url)\n",
    "        except:\n",
    "            break\n",
    "        \n",
    "        if response.status_code != 200:\n",
    "            break\n",
    "           \n",
    "\n",
    "        doc = BeautifulSoup(response.text, 'html.parser')\n",
    "        titles = get_movie_titles(doc)\n",
    "        years = get_movie_year(doc)\n",
    "        ratings = get_movie_rating(doc)\n",
    "    \n",
    "        for i in range(len(titles)):\n",
    "            movies_dict['title'].append(titles[i])\n",
    "            movies_dict['year'].append(years[i])\n",
    "            movies_dict['rating'].append(ratings[i])\n",
    "        \n",
    "    return pd.DataFrame(movies_dict)\n",
    "\n",
    "movies_dict = {'Title':titles,'Year':years,'Rating':ratings,}\n",
    "\n",
    "df = pd.DataFrame(movies_dict)\n",
    "df\n",
    "df[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c8325b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a83e06c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "29ad52b6",
   "metadata": {},
   "source": [
    "# 3.Python program to display IMDB’s Top rated 100 Indian movies’ data \n",
    "Scrapping Name, Rating, Year of Release and its make data frame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82340ba4",
   "metadata": {},
   "source": [
    "# Scrapping as follows :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "84c4f2cb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in c:\\users\\clickmoney\\anaconda3\\lib\\site-packages (2.26.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\clickmoney\\anaconda3\\lib\\site-packages (from requests) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\clickmoney\\anaconda3\\lib\\site-packages (from requests) (3.2)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\clickmoney\\anaconda3\\lib\\site-packages (from requests) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\clickmoney\\anaconda3\\lib\\site-packages (from requests) (1.26.7)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\clickmoney\\anaconda3\\lib\\site-packages (4.10.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\clickmoney\\anaconda3\\lib\\site-packages (from beautifulsoup4) (2.2.1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Movie Title</th>\n",
       "      <th>Year</th>\n",
       "      <th>Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Rocketry: The Nambi Effect</td>\n",
       "      <td>2022)</td>\n",
       "      <td>8.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Anbe Sivam</td>\n",
       "      <td>2003)</td>\n",
       "      <td>8.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Golmaal</td>\n",
       "      <td>1979)</td>\n",
       "      <td>8.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Nayakan</td>\n",
       "      <td>1987)</td>\n",
       "      <td>8.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Jai Bhim</td>\n",
       "      <td>2021)</td>\n",
       "      <td>8.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>Angoor</td>\n",
       "      <td>1982)</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>Rang De Basanti</td>\n",
       "      <td>2006)</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>Baahubali 2: The Conclusion</td>\n",
       "      <td>2017)</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>Baasha</td>\n",
       "      <td>1995)</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>Virumandi</td>\n",
       "      <td>2004)</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Movie Title   Year Rating\n",
       "0    Rocketry: The Nambi Effect  2022)    8.5\n",
       "1                    Anbe Sivam  2003)    8.4\n",
       "2                       Golmaal  1979)    8.4\n",
       "3                       Nayakan  1987)    8.4\n",
       "4                      Jai Bhim  2021)    8.4\n",
       "..                          ...    ...    ...\n",
       "95                       Angoor  1982)    8.0\n",
       "96              Rang De Basanti  2006)    8.0\n",
       "97  Baahubali 2: The Conclusion  2017)    8.0\n",
       "98                       Baasha  1995)    8.0\n",
       "99                    Virumandi  2004)    8.0\n",
       "\n",
       "[100 rows x 3 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip install requests\n",
    "import requests\n",
    "\n",
    "topics_url = 'https://www.imdb.com/india/top-rated-indian-movies/'\n",
    "response = requests.get(topics_url)\n",
    "\n",
    "page_content = response.text\n",
    "page_content\n",
    "\n",
    "!pip install beautifulsoup4\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "doc = BeautifulSoup(response.text, 'html.parser')\n",
    "doc.find('title')\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def get_topics_page():\n",
    "    topic_url = 'https://www.imdb.com/india/top-rated-indian-movies/'\n",
    "    response=requests.get(topic_url)\n",
    "    \n",
    "    if response.status_code != 200:\n",
    "        raise Exception(f'Failed to load page {topic_url}')\n",
    "    \n",
    "    doc = BeautifulSoup(response.text, 'html.parser')\n",
    "    return doc\n",
    "\n",
    "doc=get_topics_page()\n",
    "\n",
    "def get_movie_titles(doc):\n",
    "    \n",
    "    selection_class=\"titleColumn\"\n",
    "    movie_title_tags=doc.find_all('td',{'class':selection_class})\n",
    "    movie_titles=[]\n",
    "\n",
    "    for tag in movie_title_tags:\n",
    "        title = tag.find('a').text\n",
    "        movie_titles.append(title)   \n",
    "        \n",
    "    return movie_titles\n",
    "\n",
    "titles = get_movie_titles(doc)\n",
    "titles[:100]\n",
    "\n",
    "def get_movie_year(doc):\n",
    "    year_selector = \"secondaryInfo\"           \n",
    "    movie_year_tags=doc.find_all('span',{'class':year_selector})\n",
    "    movie_year_tagss=[]\n",
    "    for tag in movie_year_tags:\n",
    "        movie_year_tagss.append(tag.get_text().strip()[1:100])\n",
    "    return movie_year_tagss\n",
    "\n",
    "years = get_movie_year(doc)\n",
    "years[:100]\n",
    "\n",
    "def get_movie_rating(doc):\n",
    "    rating_selector=\"ratingColumn imdbRating\"         \n",
    "    movie_rating_tags=doc.find_all('td',{'class':rating_selector})\n",
    "    movie_rating_tagss=[]\n",
    "    for tag in movie_rating_tags:\n",
    "        movie_rating_tagss.append(tag.get_text().strip())\n",
    "    return movie_rating_tagss\n",
    "ratings = get_movie_rating(doc)\n",
    "ratings[:100]\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "def all_pages():\n",
    "\n",
    "    movies_dict={'title':[],'year':[],'rating':[]}\n",
    "  \n",
    "    for i in range(1,2000,100):\n",
    "       \n",
    "        try:\n",
    "            url = 'https://www.imdb.com/india/top-rated-indian-movies/'\n",
    "            response = requests.get(url)\n",
    "        except:\n",
    "            break\n",
    "        \n",
    "        if response.status_code != 200:\n",
    "            break\n",
    "           \n",
    "\n",
    "        doc = BeautifulSoup(response.text, 'html.parser')\n",
    "        titles = get_movie_titles(doc)\n",
    "        years = get_movie_year(doc)\n",
    "        ratings = get_movie_rating(doc)\n",
    "    \n",
    "        for i in range(len(titles)):\n",
    "            movies_dict['title'].append(titles[i])\n",
    "            movies_dict['year'].append(years[i])\n",
    "            movies_dict['rating'].append(ratings[i])\n",
    "        \n",
    "    return pd.DataFrame(movies_dict)\n",
    "movies_dict = {'Movie Title':titles,'Year':years,'Rating':ratings}\n",
    "df = pd.DataFrame(movies_dict)\n",
    "df\n",
    "df[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f918d34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e8745c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "aa063b14",
   "metadata": {},
   "source": [
    "# Q4.Python program to display list of respected former presidents of India\n",
    "1)Name of the President\n",
    "\n",
    "2)Term of office from https://presidentofindia.nic.in/former-presidents.htm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15829f86",
   "metadata": {},
   "source": [
    "# Scrapping as follows :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1d000696",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bs4 in c:\\users\\clickmoney\\anaconda3\\lib\\site-packages (0.0.1)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\clickmoney\\anaconda3\\lib\\site-packages (from bs4) (4.10.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\clickmoney\\anaconda3\\lib\\site-packages (from beautifulsoup4->bs4) (2.2.1)\n",
      "Requirement already satisfied: requests in c:\\users\\clickmoney\\anaconda3\\lib\\site-packages (2.26.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\clickmoney\\anaconda3\\lib\\site-packages (from requests) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\clickmoney\\anaconda3\\lib\\site-packages (from requests) (3.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\clickmoney\\anaconda3\\lib\\site-packages (from requests) (1.26.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\clickmoney\\anaconda3\\lib\\site-packages (from requests) (2.0.4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Presidents_name</th>\n",
       "      <th>Term_of_Office</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Shri Ram Nath Kovind (birth - 1945)</td>\n",
       "      <td>25 July, 2017 to 25 July, 2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Shri Pranab Mukherjee (1935-2020)</td>\n",
       "      <td>25 July, 2012 to 25 July, 2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Smt Pratibha Devisingh Patil (birth - 1934)</td>\n",
       "      <td>25 July, 2007 to 25 July, 2012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DR. A.P.J. Abdul Kalam (1931-2015)</td>\n",
       "      <td>25 July, 2002 to 25 July, 2007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Shri K. R. Narayanan (1920 - 2005)</td>\n",
       "      <td>25 July, 1997 to 25 July, 2002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Dr Shankar Dayal Sharma (1918-1999)</td>\n",
       "      <td>25 July, 1992 to 25 July, 1997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Shri R Venkataraman (1910-2009)</td>\n",
       "      <td>25 July, 1987 to 25 July, 1992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Giani Zail Singh (1916-1994)</td>\n",
       "      <td>25 July, 1982 to 25 July, 1987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Shri Neelam Sanjiva Reddy (1913-1996)</td>\n",
       "      <td>25 July, 1977 to 25 July, 1982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Dr. Fakhruddin Ali Ahmed (1905-1977)</td>\n",
       "      <td>24 August, 1974 to 11 February, 1977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Shri Varahagiri Venkata Giri (1894-1980)</td>\n",
       "      <td>3 May, 1969 to 20 July, 1969 and 24 August, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Dr. Zakir Husain (1897-1969)</td>\n",
       "      <td>13 May, 1967 to 3 May, 1969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Dr. Sarvepalli Radhakrishnan (1888-1975)</td>\n",
       "      <td>13 May, 1962 to 13 May, 1967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Dr. Rajendra Prasad (1884-1963)</td>\n",
       "      <td>26 January, 1950 to 13 May, 1962</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                Presidents_name  \\\n",
       "0           Shri Ram Nath Kovind (birth - 1945)   \n",
       "1             Shri Pranab Mukherjee (1935-2020)   \n",
       "2   Smt Pratibha Devisingh Patil (birth - 1934)   \n",
       "3            DR. A.P.J. Abdul Kalam (1931-2015)   \n",
       "4            Shri K. R. Narayanan (1920 - 2005)   \n",
       "5           Dr Shankar Dayal Sharma (1918-1999)   \n",
       "6               Shri R Venkataraman (1910-2009)   \n",
       "7                  Giani Zail Singh (1916-1994)   \n",
       "8         Shri Neelam Sanjiva Reddy (1913-1996)   \n",
       "9          Dr. Fakhruddin Ali Ahmed (1905-1977)   \n",
       "10     Shri Varahagiri Venkata Giri (1894-1980)   \n",
       "11                 Dr. Zakir Husain (1897-1969)   \n",
       "12     Dr. Sarvepalli Radhakrishnan (1888-1975)   \n",
       "13             Dr. Rajendra Prasad (1884-1963)    \n",
       "\n",
       "                                       Term_of_Office  \n",
       "0                     25 July, 2017 to 25 July, 2022   \n",
       "1                     25 July, 2012 to 25 July, 2017   \n",
       "2                     25 July, 2007 to 25 July, 2012   \n",
       "3                     25 July, 2002 to 25 July, 2007   \n",
       "4                     25 July, 1997 to 25 July, 2002   \n",
       "5                     25 July, 1992 to 25 July, 1997   \n",
       "6                     25 July, 1987 to 25 July, 1992   \n",
       "7                     25 July, 1982 to 25 July, 1987   \n",
       "8                     25 July, 1977 to 25 July, 1982   \n",
       "9                24 August, 1974 to 11 February, 1977  \n",
       "10   3 May, 1969 to 20 July, 1969 and 24 August, 1...  \n",
       "11                        13 May, 1967 to 3 May, 1969  \n",
       "12                       13 May, 1962 to 13 May, 1967  \n",
       "13                   26 January, 1950 to 13 May, 1962  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip install bs4\n",
    "!pip install requests\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "prpage = requests.get(\"https://presidentofindia.nic.in/former-presidents.htm\")\n",
    "prpage\n",
    "\n",
    "soup=BeautifulSoup(prpage.content)\n",
    "prlist =soup.find_all('div',class_=\"presidentListing\")\n",
    "\n",
    "Name =[]\n",
    "office =[]\n",
    "for content in prlist:\n",
    "    president = content.h3.text\n",
    "    Name.append(president)\n",
    "    term=content.p.text.replace('Term of Office:','')\n",
    "    office.append(term)\n",
    "president = pd.DataFrame(list(zip(Name,office)),columns=['Presidents_name','Term_of_Office'])\n",
    "president"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3072cc4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e694b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "160220a6",
   "metadata": {},
   "source": [
    "# Q5.Python program to scrape cricket rankings from icc-cricket.com.\n",
    "\n",
    "Scrape the following\n",
    "\n",
    "a) Top 10 ODI teams in men’s cricket along with the records for matches, points and rating.\n",
    "\n",
    "b) Top 10 ODI Batsmen along with the records of their team and rating.\n",
    "\n",
    "c) Top 10 ODI bowlers along with the records of their team and rating"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c0bb433",
   "metadata": {},
   "source": [
    "# Scraping as follows :\n",
    "    \n",
    "a) Top 10 ODI teams in men’s cricket along with the records for matches, points and rating.    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9f7b611c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Team_name</th>\n",
       "      <th>Matches</th>\n",
       "      <th>Points</th>\n",
       "      <th>Ratings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>New Zealand</td>\n",
       "      <td>19</td>\n",
       "      <td>2,355</td>\n",
       "      <td>124               ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>England</td>\n",
       "      <td>27</td>\n",
       "      <td>3,226</td>\n",
       "      <td>119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>India</td>\n",
       "      <td>31</td>\n",
       "      <td>3,447</td>\n",
       "      <td>111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Pakistan</td>\n",
       "      <td>22</td>\n",
       "      <td>2,354</td>\n",
       "      <td>107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Australia</td>\n",
       "      <td>25</td>\n",
       "      <td>2,548</td>\n",
       "      <td>102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>South Africa</td>\n",
       "      <td>21</td>\n",
       "      <td>2,111</td>\n",
       "      <td>101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Bangladesh</td>\n",
       "      <td>30</td>\n",
       "      <td>2,753</td>\n",
       "      <td>92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Sri Lanka</td>\n",
       "      <td>29</td>\n",
       "      <td>2,658</td>\n",
       "      <td>92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>West Indies</td>\n",
       "      <td>41</td>\n",
       "      <td>2,902</td>\n",
       "      <td>71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>18</td>\n",
       "      <td>1,238</td>\n",
       "      <td>69</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Team_name Matches Points  \\\n",
       "0   New Zealand      19  2,355   \n",
       "1       England      27  3,226   \n",
       "2         India      31  3,447   \n",
       "3      Pakistan      22  2,354   \n",
       "4     Australia      25  2,548   \n",
       "5  South Africa      21  2,111   \n",
       "6    Bangladesh      30  2,753   \n",
       "7     Sri Lanka      29  2,658   \n",
       "8   West Indies      41  2,902   \n",
       "9   Afghanistan      18  1,238   \n",
       "\n",
       "                                             Ratings  \n",
       "0                              124               ...  \n",
       "1                                                119  \n",
       "2                                                111  \n",
       "3                                                107  \n",
       "4                                                102  \n",
       "5                                                101  \n",
       "6                                                 92  \n",
       "7                                                 92  \n",
       "8                                                 71  \n",
       "9                                                 69  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "url = \"https://www.icc-cricket.com/rankings/mens/team-rankings/odi\"\n",
    "page = requests.get(url)\n",
    "\n",
    "soup10 = BeautifulSoup(page.content)\n",
    "\n",
    "team = soup10.find_all(\"span\",class_='u-hide-phablet')\n",
    "team_name = []\n",
    "for i in team:\n",
    "    team_name.append(i.text)\n",
    "matches = [] \n",
    "points = [] \n",
    "ratings = [] \n",
    "final_list = []\n",
    "\n",
    "for i in soup10.find_all(\"td\",class_='rankings-block__banner--matches'): \n",
    "    matches.append(i.text)\n",
    "    \n",
    "for i in soup10.find_all(\"td\",class_='rankings-block__banner--points'):\n",
    "    points.append(i.text)\n",
    "    \n",
    "for i in soup10.find_all(\"td\",class_='rankings-block__banner--rating u-text-right'):\n",
    "    ratings.append(i.text.replace(\"\\n\",\"\"))\n",
    "    \n",
    "for i in soup10.find_all(\"td\",class_='table-body__cell u-center-text'):\n",
    "    final_list.append(i.text)\n",
    "    \n",
    "for i in range(0,len(final_list)-1,2):\n",
    "    matches.append(final_list[i]) \n",
    "    points.append(final_list[i+1])\n",
    "for i in soup10.find_all(\"td\",class_='table-body__cell u-text-right rating'):\n",
    "    ratings.append(i.text)\n",
    "    \n",
    "data = list(zip(team_name,matches,points,ratings))\n",
    "df = pd.DataFrame(data,columns=[\"Team_name\",\"Matches\",\"Points\",\"Ratings\"])\n",
    "df \n",
    "df[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "617b05c4",
   "metadata": {},
   "source": [
    "b) Top 10 ODI Batsmen along with the records of their team and rating."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "835723af",
   "metadata": {},
   "source": [
    "# Scrapping as follows :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e04e5c5d",
   "metadata": {},
   "source": [
    "b) Top 10 ODI Batsmen along with the records of their team and rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9c3c43db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Batsmen</th>\n",
       "      <th>Team_name</th>\n",
       "      <th>Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Babar Azam</td>\n",
       "      <td>PAK</td>\n",
       "      <td>890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Rassie van der Dussen</td>\n",
       "      <td>SA</td>\n",
       "      <td>789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Quinton de Kock</td>\n",
       "      <td>SA</td>\n",
       "      <td>784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Imam-ul-Haq</td>\n",
       "      <td>PAK</td>\n",
       "      <td>779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Virat Kohli</td>\n",
       "      <td>IND</td>\n",
       "      <td>744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Rohit Sharma</td>\n",
       "      <td>IND</td>\n",
       "      <td>740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>David Warner</td>\n",
       "      <td>AUS</td>\n",
       "      <td>739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Jonny Bairstow</td>\n",
       "      <td>ENG</td>\n",
       "      <td>732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Ross Taylor</td>\n",
       "      <td>NZ</td>\n",
       "      <td>722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Aaron Finch</td>\n",
       "      <td>AUS</td>\n",
       "      <td>706</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Batsmen                Team_name Rating\n",
       "0             Babar Azam  PAK                        890\n",
       "1  Rassie van der Dussen                       SA    789\n",
       "2        Quinton de Kock                       SA    784\n",
       "3            Imam-ul-Haq                      PAK    779\n",
       "4            Virat Kohli                      IND    744\n",
       "5           Rohit Sharma                      IND    740\n",
       "6           David Warner                      AUS    739\n",
       "7         Jonny Bairstow                      ENG    732\n",
       "8            Ross Taylor                       NZ    722\n",
       "9            Aaron Finch                      AUS    706"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "url = \"https://www.icc-cricket.com/rankings/mens/player-rankings/odi/batting\"\n",
    "page = requests.get(url)\n",
    "\n",
    "soup11 = BeautifulSoup(page.content)\n",
    "batsmen = [] \n",
    "team_name = [] \n",
    "rating = [] \n",
    "\n",
    "for i in soup11.find_all(\"div\",class_='rankings-block__banner--name-large'): \n",
    "    batsmen.append(i.text)\n",
    "for i in soup11.find_all(\"div\",class_='rankings-block__banner--nationality'): \n",
    "    team_name.append(i.text.replace(\"\\n\",\"\"))\n",
    "for i in soup11.find_all(\"div\",class_='rankings-block__banner--rating'): \n",
    "    rating.append(i.text)\n",
    "for i in soup11.find_all(\"td\",class_='table-body__cell rankings-table__name name'):\n",
    "    for j in i.find_all('a'):\n",
    "        batsmen.append(j.text)\n",
    "for i in soup11.find_all(\"span\",class_='table-body__logo-text'): \n",
    "    team_name.append(i.text)\n",
    "for i in soup11.find_all(\"td\",class_='table-body__cell rating'): \n",
    "    rating.append(i.text)\n",
    "\n",
    "data = list(zip(batsmen,team_name,rating))\n",
    "df = pd.DataFrame(data,columns=[\"Batsmen\",\"Team_name\",\"Rating\"])\n",
    "df\n",
    "df[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c3f8dd",
   "metadata": {},
   "source": [
    "# Scrapping as follows :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46036270",
   "metadata": {},
   "source": [
    "c) Top 10 ODI bowlers along with the records of their team and rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "61e08933",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Bowler</th>\n",
       "      <th>Team_name</th>\n",
       "      <th>Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Trent Boult</td>\n",
       "      <td>NZ</td>\n",
       "      <td>720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Josh Hazlewood</td>\n",
       "      <td>AUS</td>\n",
       "      <td>678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Mujeeb Ur Rahman</td>\n",
       "      <td>AFG</td>\n",
       "      <td>676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Jasprit Bumrah</td>\n",
       "      <td>IND</td>\n",
       "      <td>662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Shaheen Afridi</td>\n",
       "      <td>PAK</td>\n",
       "      <td>661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Mohammad Nabi</td>\n",
       "      <td>AFG</td>\n",
       "      <td>657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Mehedi Hasan</td>\n",
       "      <td>BAN</td>\n",
       "      <td>655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Rashid Khan</td>\n",
       "      <td>AFG</td>\n",
       "      <td>651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Matt Henry</td>\n",
       "      <td>NZ</td>\n",
       "      <td>644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Mustafizur Rahman</td>\n",
       "      <td>BAN</td>\n",
       "      <td>640</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Bowler               Team_name Rating\n",
       "0        Trent Boult  NZ                        720\n",
       "1     Josh Hazlewood                     AUS    678\n",
       "2   Mujeeb Ur Rahman                     AFG    676\n",
       "3     Jasprit Bumrah                     IND    662\n",
       "4     Shaheen Afridi                     PAK    661\n",
       "5      Mohammad Nabi                     AFG    657\n",
       "6       Mehedi Hasan                     BAN    655\n",
       "7        Rashid Khan                     AFG    651\n",
       "8         Matt Henry                      NZ    644\n",
       "9  Mustafizur Rahman                     BAN    640"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "url = \"https://www.icc-cricket.com/rankings/mens/player-rankings/odi/bowling\"\n",
    "page = requests.get(url)\n",
    "\n",
    "soup12 = BeautifulSoup(page.content)\n",
    "\n",
    "bowler = [] \n",
    "team_name = [] \n",
    "rating = [] \n",
    "\n",
    "for i in soup12.find_all(\"div\",class_='rankings-block__banner--name-large'): \n",
    "    bowler.append(i.text)\n",
    "for i in soup12.find_all(\"div\",class_='rankings-block__banner--nationality'): \n",
    "    team_name.append(i.text.replace(\"\\n\",\"\"))\n",
    "for i in soup12.find_all(\"div\",class_='rankings-block__banner--rating'): \n",
    "    rating.append(i.text)\n",
    "for i in soup12.find_all(\"td\",class_='table-body__cell rankings-table__name name'):\n",
    "    for j in i.find_all('a'):\n",
    "        bowler.append(j.text)\n",
    "for i in soup12.find_all(\"span\",class_='table-body__logo-text'): \n",
    "    team_name.append(i.text)\n",
    "for i in soup12.find_all(\"td\",class_='table-body__cell rating'): \n",
    "    rating.append(i.text)\n",
    "\n",
    "data = list(zip(bowler,team_name,rating))\n",
    "df = pd.DataFrame(data,columns=[\"Bowler\",\"Team_name\",\"Rating\"])\n",
    "df[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e10876",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9274db72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "79715a9b",
   "metadata": {},
   "source": [
    "# Q6.Python program to scrape cricket rankings from icc-cricket.com"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be24bef",
   "metadata": {},
   "source": [
    "Scrape\n",
    "\n",
    "a) Top 10 ODI teams in women’s cricket along with the records for matches, points and rating.\n",
    "\n",
    "b) Top 10 women’s ODI Batting players along with the records of their team and rating.\n",
    "\n",
    "c) Top 10 women’s ODI all-rounder along with the records of their team and rating"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb17a59f",
   "metadata": {},
   "source": [
    "# Scrapping as follows :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd34d46",
   "metadata": {},
   "source": [
    "a) Top 10 ODI teams in women’s cricket along with the records for matches, points and rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "68d22768",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Team_name</th>\n",
       "      <th>Matches</th>\n",
       "      <th>Points</th>\n",
       "      <th>Ratings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Australia</td>\n",
       "      <td>29</td>\n",
       "      <td>4,837</td>\n",
       "      <td>167               ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>England</td>\n",
       "      <td>33</td>\n",
       "      <td>4,046</td>\n",
       "      <td>123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>South Africa</td>\n",
       "      <td>35</td>\n",
       "      <td>4,157</td>\n",
       "      <td>119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>India</td>\n",
       "      <td>32</td>\n",
       "      <td>3,219</td>\n",
       "      <td>101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>New Zealand</td>\n",
       "      <td>31</td>\n",
       "      <td>3,019</td>\n",
       "      <td>97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>West Indies</td>\n",
       "      <td>30</td>\n",
       "      <td>2,768</td>\n",
       "      <td>92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Bangladesh</td>\n",
       "      <td>12</td>\n",
       "      <td>930</td>\n",
       "      <td>78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Pakistan</td>\n",
       "      <td>30</td>\n",
       "      <td>1,962</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Ireland</td>\n",
       "      <td>11</td>\n",
       "      <td>516</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Sri Lanka</td>\n",
       "      <td>11</td>\n",
       "      <td>495</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Team_name Matches Points  \\\n",
       "0     Australia      29  4,837   \n",
       "1       England      33  4,046   \n",
       "2  South Africa      35  4,157   \n",
       "3         India      32  3,219   \n",
       "4   New Zealand      31  3,019   \n",
       "5   West Indies      30  2,768   \n",
       "6    Bangladesh      12    930   \n",
       "7      Pakistan      30  1,962   \n",
       "8       Ireland      11    516   \n",
       "9     Sri Lanka      11    495   \n",
       "\n",
       "                                             Ratings  \n",
       "0                              167               ...  \n",
       "1                                                123  \n",
       "2                                                119  \n",
       "3                                                101  \n",
       "4                                                 97  \n",
       "5                                                 92  \n",
       "6                                                 78  \n",
       "7                                                 65  \n",
       "8                                                 47  \n",
       "9                                                 45  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "url = \"https://www.icc-cricket.com/rankings/womens/team-rankings/odi\"\n",
    "page = requests.get(url)\n",
    "soup14 = BeautifulSoup(page.content)\n",
    "womens_team = soup14.find_all(\"span\",class_='u-hide-phablet')\n",
    "womens_team_name = []\n",
    "for i in womens_team:\n",
    "    womens_team_name.append(i.text)\n",
    "womens_matches = [] \n",
    "womens_points = [] \n",
    "womens_ratings = [] \n",
    "womens_final_list = [] \n",
    "for i in soup14.find_all(\"td\",class_='rankings-block__banner--matches'): \n",
    "    womens_matches.append(i.text)\n",
    "for i in soup14.find_all(\"td\",class_='rankings-block__banner--points'):\n",
    "    womens_points.append(i.text)\n",
    "for i in soup14.find_all(\"td\",class_='rankings-block__banner--rating u-text-right'):\n",
    "    womens_ratings.append(i.text.replace(\"\\n\",\"\"))\n",
    "for i in soup14.find_all(\"td\",class_='table-body__cell u-center-text'):\n",
    "    womens_final_list.append(i.text)\n",
    "for i in range(0,len(womens_final_list)-1,2):\n",
    "    womens_matches.append(womens_final_list[i]) \n",
    "    womens_points.append(womens_final_list[i+1]) \n",
    "for i in soup14.find_all(\"td\",class_='table-body__cell u-text-right rating'):\n",
    "    womens_ratings.append(i.text)\n",
    "\n",
    "\n",
    "data = list(zip(womens_team_name,womens_matches,womens_points,womens_ratings))\n",
    "df = pd.DataFrame(data,columns=[\"Team_name\",\"Matches\",\"Points\",\"Ratings\"])\n",
    "df\n",
    "df[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b31506",
   "metadata": {},
   "source": [
    "b) Top 10 women’s ODI Batting players along with the records of their team and rating."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d7e1481",
   "metadata": {},
   "source": [
    "# Scrapping as follows :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8d2df770",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Batting_Player</th>\n",
       "      <th>Team_name</th>\n",
       "      <th>Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Alyssa Healy</td>\n",
       "      <td>AUS</td>\n",
       "      <td>785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Beth Mooney</td>\n",
       "      <td>AUS</td>\n",
       "      <td>749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Natalie Sciver</td>\n",
       "      <td>ENG</td>\n",
       "      <td>747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Laura Wolvaardt</td>\n",
       "      <td>SA</td>\n",
       "      <td>732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Meg Lanning</td>\n",
       "      <td>AUS</td>\n",
       "      <td>710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Rachael Haynes</td>\n",
       "      <td>AUS</td>\n",
       "      <td>701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Amy Satterthwaite</td>\n",
       "      <td>NZ</td>\n",
       "      <td>681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Tammy Beaumont</td>\n",
       "      <td>ENG</td>\n",
       "      <td>667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Chamari Athapaththu</td>\n",
       "      <td>SL</td>\n",
       "      <td>655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Smriti Mandhana</td>\n",
       "      <td>IND</td>\n",
       "      <td>649</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Batting_Player                Team_name Rating\n",
       "0         Alyssa Healy  AUS                        785\n",
       "1          Beth Mooney                      AUS    749\n",
       "2       Natalie Sciver                      ENG    747\n",
       "3      Laura Wolvaardt                       SA    732\n",
       "4          Meg Lanning                      AUS    710\n",
       "5       Rachael Haynes                      AUS    701\n",
       "6    Amy Satterthwaite                       NZ    681\n",
       "7       Tammy Beaumont                      ENG    667\n",
       "8  Chamari Athapaththu                       SL    655\n",
       "9      Smriti Mandhana                      IND    649"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "url = \"https://www.icc-cricket.com/rankings/womens/player-rankings/odi/batting\"\n",
    "page = requests.get(url)\n",
    "soup15 = BeautifulSoup(page.content)\n",
    "players = [] \n",
    "team_name = [] \n",
    "rating = [] \n",
    "\n",
    "for i in soup15.find_all(\"div\",class_='rankings-block__banner--name-large'): \n",
    "    players.append(i.text)\n",
    "for i in soup15.find_all(\"div\",class_='rankings-block__banner--nationality'):\n",
    "    team_name.append(i.text.replace(\"\\n\",\"\"))\n",
    "for i in soup15.find_all(\"div\",class_='rankings-block__banner--rating'): \n",
    "    rating.append(i.text)\n",
    "for i in soup15.find_all(\"td\",class_='table-body__cell rankings-table__name name'):\n",
    "    for j in i.find_all('a'):\n",
    "        players.append(j.text)\n",
    "for i in soup15.find_all(\"span\",class_='table-body__logo-text'): \n",
    "    team_name.append(i.text)\n",
    "for i in soup15.find_all(\"td\",class_='table-body__cell rating'): \n",
    "    rating.append(i.text)\n",
    "\n",
    "top_players=pd.DataFrame({})\n",
    "top_players['Batting_Player']=players[:10]\n",
    "top_players['Team']=team_name[:10]\n",
    "top_players['Rating']=rating[:10]\n",
    "top_players\n",
    "\n",
    "data = list(zip(players,team_name,rating))\n",
    "df = pd.DataFrame(data,columns=[\"Batting_Player\",\"Team_name\",\"Rating\"])\n",
    "df[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eef4909",
   "metadata": {},
   "source": [
    "c) Top 10 women’s ODI all-rounder along with the records of their team and rating"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed8e555e",
   "metadata": {},
   "source": [
    "# Scrapping as follows :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "81d06d59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>All_Rounder</th>\n",
       "      <th>Team_name</th>\n",
       "      <th>Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Natalie Sciver</td>\n",
       "      <td>ENG</td>\n",
       "      <td>379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ellyse Perry</td>\n",
       "      <td>AUS</td>\n",
       "      <td>374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Marizanne Kapp</td>\n",
       "      <td>SA</td>\n",
       "      <td>349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Hayley Matthews</td>\n",
       "      <td>WI</td>\n",
       "      <td>339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Amelia Kerr</td>\n",
       "      <td>NZ</td>\n",
       "      <td>336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Ashleigh Gardner</td>\n",
       "      <td>AUS</td>\n",
       "      <td>270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Deepti Sharma</td>\n",
       "      <td>IND</td>\n",
       "      <td>252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Jess Jonassen</td>\n",
       "      <td>AUS</td>\n",
       "      <td>246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Katherine Brunt</td>\n",
       "      <td>ENG</td>\n",
       "      <td>220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Stafanie Taylor</td>\n",
       "      <td>WI</td>\n",
       "      <td>207</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        All_Rounder                Team_name Rating\n",
       "0    Natalie Sciver  ENG                        379\n",
       "1      Ellyse Perry                      AUS    374\n",
       "2    Marizanne Kapp                       SA    349\n",
       "3   Hayley Matthews                       WI    339\n",
       "4       Amelia Kerr                       NZ    336\n",
       "5  Ashleigh Gardner                      AUS    270\n",
       "6     Deepti Sharma                      IND    252\n",
       "7     Jess Jonassen                      AUS    246\n",
       "8   Katherine Brunt                      ENG    220\n",
       "9   Stafanie Taylor                       WI    207"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "url = \"https://www.icc-cricket.com/rankings/womens/player-rankings/odi/all-rounder\"\n",
    "page = requests.get(url)\n",
    "soup16 = BeautifulSoup(page.content)\n",
    "players = [] \n",
    "team_name = []   \n",
    "rating = []  \n",
    "\n",
    "for i in soup16.find_all(\"div\",class_='rankings-block__banner--name-large'): \n",
    "    players.append(i.text)\n",
    "for i in soup16.find_all(\"div\",class_='rankings-block__banner--nationality'): \n",
    "    team_name.append(i.text.replace(\"\\n\",\"\"))\n",
    "for i in soup16.find_all(\"div\",class_='rankings-block__banner--rating'): \n",
    "    rating.append(i.text)\n",
    "for i in soup16.find_all(\"td\",class_='table-body__cell rankings-table__name name'):\n",
    "    for j in i.find_all('a'):\n",
    "        players.append(j.text)\n",
    "for i in soup16.find_all(\"span\",class_='table-body__logo-text'): \n",
    "    team_name.append(i.text)\n",
    "for i in soup16.find_all(\"td\",class_='table-body__cell rating'): \n",
    "    rating.append(i.text)\n",
    "\n",
    "all_rounder=pd.DataFrame({})\n",
    "all_rounder['All_Rounder']=players[:10]\n",
    "all_rounder['Team']=team_name[:10]\n",
    "all_rounder['Rating']=rating[:10]\n",
    "all_rounder\n",
    "\n",
    "data = list(zip(players,team_name,rating))\n",
    "df = pd.DataFrame(data,columns=[\"All_Rounder\",\"Team_name\",\"Rating\"])\n",
    "df\n",
    "df[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c4f68f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd64d527",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b2b6c86f",
   "metadata": {},
   "source": [
    "# Q7.Python program to scrape mentioned news details from https://www.cnbc.com/world/?region=world\n",
    "\n",
    "i) Headline\n",
    "\n",
    "ii) Time\n",
    "\n",
    "iii) News Link"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe3011d",
   "metadata": {},
   "source": [
    "# Scrapping as follows :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fd20b37b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bs4 in c:\\users\\clickmoney\\anaconda3\\lib\\site-packages (0.0.1)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\clickmoney\\anaconda3\\lib\\site-packages (from bs4) (4.10.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\clickmoney\\anaconda3\\lib\\site-packages (from beautifulsoup4->bs4) (2.2.1)\n",
      "Requirement already satisfied: requests in c:\\users\\clickmoney\\anaconda3\\lib\\site-packages (2.26.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\clickmoney\\anaconda3\\lib\\site-packages (from requests) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\clickmoney\\anaconda3\\lib\\site-packages (from requests) (1.26.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\clickmoney\\anaconda3\\lib\\site-packages (from requests) (3.2)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\clickmoney\\anaconda3\\lib\\site-packages (from requests) (2.0.4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Headline</th>\n",
       "      <th>Time</th>\n",
       "      <th>News_link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This portrait of Warren Buffett just sold for ...</td>\n",
       "      <td>51 Min Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/08/31/warren-buffett...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>This portrait of Warren Buffett just sold for ...</td>\n",
       "      <td>51 Min Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/08/31/microstrategy-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Microstrategy chairman Michael Saylor accused ...</td>\n",
       "      <td>1 Hour Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/08/31/were-making-a-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>We're making a buy in this down market but als...</td>\n",
       "      <td>1 Hour Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/08/31/bernsteins-tra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bernstein's trading strategy for internet stoc...</td>\n",
       "      <td>1 Hour Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/08/31/despite-recess...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Despite recession fears, most 401(k) investors...</td>\n",
       "      <td>2 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/08/31/trump-lawyer-j...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Trump lawyer Eastman takes Fifth before Georgi...</td>\n",
       "      <td>2 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/08/31/these-stocks-a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>These stocks are underappreciated beneficiarie...</td>\n",
       "      <td>2 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/08/31/jeff-bezos-son...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Jeff Bezos reveals warning from his son for Am...</td>\n",
       "      <td>2 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/08/31/identity-crime...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Identity scams are at an all-time high. Here a...</td>\n",
       "      <td>2 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/08/31/these-bank-fee...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>These bank fees could be taking a bite out of ...</td>\n",
       "      <td>2 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/08/31/irs-is-not-hir...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>The IRS isn't hiring an 'army' of auditors—her...</td>\n",
       "      <td>2 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/08/31/44percent-of-a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Six in 10 adults want to become a billionaire ...</td>\n",
       "      <td>3 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/08/31/market-headwin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Market headwinds could worsen as stocks enter ...</td>\n",
       "      <td>3 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/08/31/walmart-sams-c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Walmart-owned Sam's Club raises annual members...</td>\n",
       "      <td>3 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/08/31/nobody-wants-t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Nobody wants to get ahead of September's parad...</td>\n",
       "      <td>4 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/08/31/stocks-making-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Stocks making the biggest moves midday: Expres...</td>\n",
       "      <td>4 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/08/31/these-kinds-of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>These kinds of stocks do well when recession f...</td>\n",
       "      <td>4 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/08/31/8-new-shows-co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>8 new shows streaming on Amazon, Disney+, Netf...</td>\n",
       "      <td>4 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/08/31/florida-gov-ro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Florida Gov. Ron DeSantis rival Charlie Crist ...</td>\n",
       "      <td>4 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/08/31/3-takeaways-fr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>3 takeaways from the Investing Club’s ‘Morning...</td>\n",
       "      <td>4 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/08/31/stricter-stand...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Stricter standards needed for carbon offsets, ...</td>\n",
       "      <td>4 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/08/31/were-adding-to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>We're adding to two stocks in an oversold mark...</td>\n",
       "      <td>4 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/08/31/small-investor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Small investors embraced market rally, Wall St...</td>\n",
       "      <td>5 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/08/31/global-bonds-t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Global bonds teeter near bear market as centra...</td>\n",
       "      <td>5 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/08/31/jeffrey-gundla...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Gundlach says yield curve inversions are 'reli...</td>\n",
       "      <td>6 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/08/31/fda-authorizes...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>FDA authorizes Covid booster shots that target...</td>\n",
       "      <td>7 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/08/31/most-affordabl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>The 7 most affordable U.S. states to retire—an...</td>\n",
       "      <td>7 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/08/31/avoid-these-re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>'Avoid these 2 resume words at all costs,' say...</td>\n",
       "      <td>7 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/08/31/klarna-losses-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Fintech firm Klarna's losses triple after aggr...</td>\n",
       "      <td>7 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/08/31/mcdonalds-us-h...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Headline         Time  \\\n",
       "0   This portrait of Warren Buffett just sold for ...   51 Min Ago   \n",
       "1   This portrait of Warren Buffett just sold for ...   51 Min Ago   \n",
       "2   Microstrategy chairman Michael Saylor accused ...   1 Hour Ago   \n",
       "3   We're making a buy in this down market but als...   1 Hour Ago   \n",
       "4   Bernstein's trading strategy for internet stoc...   1 Hour Ago   \n",
       "5   Despite recession fears, most 401(k) investors...  2 Hours Ago   \n",
       "6   Trump lawyer Eastman takes Fifth before Georgi...  2 Hours Ago   \n",
       "7   These stocks are underappreciated beneficiarie...  2 Hours Ago   \n",
       "8   Jeff Bezos reveals warning from his son for Am...  2 Hours Ago   \n",
       "9   Identity scams are at an all-time high. Here a...  2 Hours Ago   \n",
       "10  These bank fees could be taking a bite out of ...  2 Hours Ago   \n",
       "11  The IRS isn't hiring an 'army' of auditors—her...  2 Hours Ago   \n",
       "12  Six in 10 adults want to become a billionaire ...  3 Hours Ago   \n",
       "13  Market headwinds could worsen as stocks enter ...  3 Hours Ago   \n",
       "14  Walmart-owned Sam's Club raises annual members...  3 Hours Ago   \n",
       "15  Nobody wants to get ahead of September's parad...  4 Hours Ago   \n",
       "16  Stocks making the biggest moves midday: Expres...  4 Hours Ago   \n",
       "17  These kinds of stocks do well when recession f...  4 Hours Ago   \n",
       "18  8 new shows streaming on Amazon, Disney+, Netf...  4 Hours Ago   \n",
       "19  Florida Gov. Ron DeSantis rival Charlie Crist ...  4 Hours Ago   \n",
       "20  3 takeaways from the Investing Club’s ‘Morning...  4 Hours Ago   \n",
       "21  Stricter standards needed for carbon offsets, ...  4 Hours Ago   \n",
       "22  We're adding to two stocks in an oversold mark...  4 Hours Ago   \n",
       "23  Small investors embraced market rally, Wall St...  5 Hours Ago   \n",
       "24  Global bonds teeter near bear market as centra...  5 Hours Ago   \n",
       "25  Gundlach says yield curve inversions are 'reli...  6 Hours Ago   \n",
       "26  FDA authorizes Covid booster shots that target...  7 Hours Ago   \n",
       "27  The 7 most affordable U.S. states to retire—an...  7 Hours Ago   \n",
       "28  'Avoid these 2 resume words at all costs,' say...  7 Hours Ago   \n",
       "29  Fintech firm Klarna's losses triple after aggr...  7 Hours Ago   \n",
       "\n",
       "                                            News_link  \n",
       "0   https://www.cnbc.com/2022/08/31/warren-buffett...  \n",
       "1   https://www.cnbc.com/2022/08/31/microstrategy-...  \n",
       "2   https://www.cnbc.com/2022/08/31/were-making-a-...  \n",
       "3   https://www.cnbc.com/2022/08/31/bernsteins-tra...  \n",
       "4   https://www.cnbc.com/2022/08/31/despite-recess...  \n",
       "5   https://www.cnbc.com/2022/08/31/trump-lawyer-j...  \n",
       "6   https://www.cnbc.com/2022/08/31/these-stocks-a...  \n",
       "7   https://www.cnbc.com/2022/08/31/jeff-bezos-son...  \n",
       "8   https://www.cnbc.com/2022/08/31/identity-crime...  \n",
       "9   https://www.cnbc.com/2022/08/31/these-bank-fee...  \n",
       "10  https://www.cnbc.com/2022/08/31/irs-is-not-hir...  \n",
       "11  https://www.cnbc.com/2022/08/31/44percent-of-a...  \n",
       "12  https://www.cnbc.com/2022/08/31/market-headwin...  \n",
       "13  https://www.cnbc.com/2022/08/31/walmart-sams-c...  \n",
       "14  https://www.cnbc.com/2022/08/31/nobody-wants-t...  \n",
       "15  https://www.cnbc.com/2022/08/31/stocks-making-...  \n",
       "16  https://www.cnbc.com/2022/08/31/these-kinds-of...  \n",
       "17  https://www.cnbc.com/2022/08/31/8-new-shows-co...  \n",
       "18  https://www.cnbc.com/2022/08/31/florida-gov-ro...  \n",
       "19  https://www.cnbc.com/2022/08/31/3-takeaways-fr...  \n",
       "20  https://www.cnbc.com/2022/08/31/stricter-stand...  \n",
       "21  https://www.cnbc.com/2022/08/31/were-adding-to...  \n",
       "22  https://www.cnbc.com/2022/08/31/small-investor...  \n",
       "23  https://www.cnbc.com/2022/08/31/global-bonds-t...  \n",
       "24  https://www.cnbc.com/2022/08/31/jeffrey-gundla...  \n",
       "25  https://www.cnbc.com/2022/08/31/fda-authorizes...  \n",
       "26  https://www.cnbc.com/2022/08/31/most-affordabl...  \n",
       "27  https://www.cnbc.com/2022/08/31/avoid-these-re...  \n",
       "28  https://www.cnbc.com/2022/08/31/klarna-losses-...  \n",
       "29  https://www.cnbc.com/2022/08/31/mcdonalds-us-h...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip install bs4\n",
    "!pip install requests\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "page = requests.get(\"https://www.cnbc.com/world/?region=world\")\n",
    "page\n",
    "\n",
    "soup=BeautifulSoup(page.content)\n",
    "soup\n",
    "\n",
    "Headline = []\n",
    "\n",
    "Headline = soup.find('a', class_=\"LatestNews-headline\")\n",
    "for i in soup.find_all('a', class_=\"LatestNews-headline\"):\n",
    "    Headline.append(i.text)\n",
    "\n",
    "Time = []\n",
    "Time = soup.find('time', class_=\"LatestNews-timestamp\")\n",
    "for i in soup.find_all('time', class_=\"LatestNews-timestamp\"):\n",
    "    Time.append(i.text)\n",
    "    \n",
    "News_link = []\n",
    "for i in soup.find_all('a',class_=\"LatestNews-headline\"):\n",
    "    News_link.append(i['href'])\n",
    "\n",
    "data = list(zip(Headline,Time,News_link))\n",
    "df = pd.DataFrame(data,columns=[\"Headline\",\"Time\",\"News_link\"])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4fe1233",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "196f0569",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8735e9c2",
   "metadata": {},
   "source": [
    "# Q8.Python program to scrape the details of most downloaded articles from AI in last 90 days.\n",
    "https://www.journals.elsevier.com/artificial-intelligence/most-downloaded-articles\n",
    "    \n",
    "Scrape below mentioned details :\n",
    "\n",
    "i) Paper Title\n",
    "\n",
    "ii) Authors\n",
    "\n",
    "iii) Published Date\n",
    "\n",
    "iv) Paper URL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab067c8a",
   "metadata": {},
   "source": [
    "# Scrapping as follows :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1668fd8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bs4 in c:\\users\\clickmoney\\anaconda3\\lib\\site-packages (0.0.1)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\clickmoney\\anaconda3\\lib\\site-packages (from bs4) (4.10.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\clickmoney\\anaconda3\\lib\\site-packages (from beautifulsoup4->bs4) (2.2.1)\n",
      "Requirement already satisfied: requests in c:\\users\\clickmoney\\anaconda3\\lib\\site-packages (2.26.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\clickmoney\\anaconda3\\lib\\site-packages (from requests) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\clickmoney\\anaconda3\\lib\\site-packages (from requests) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\clickmoney\\anaconda3\\lib\\site-packages (from requests) (1.26.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\clickmoney\\anaconda3\\lib\\site-packages (from requests) (3.2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Paper_titles</th>\n",
       "      <th>Authors_names</th>\n",
       "      <th>Published_date</th>\n",
       "      <th>Paper_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Reward is enough</td>\n",
       "      <td>Silver, David, Singh, Satinder, Precup, Doina,...</td>\n",
       "      <td>[October 2021]</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Reward is enough</td>\n",
       "      <td>Silver, David, Singh, Satinder, Precup, Doina,...</td>\n",
       "      <td>October 2021</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Making sense of raw input</td>\n",
       "      <td>Evans, Richard, Bošnjak, Matko and 5 more</td>\n",
       "      <td>October 2021</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Law and logic: A review from an argumentation ...</td>\n",
       "      <td>Prakken, Henry, Sartor, Giovanni</td>\n",
       "      <td>October 2015</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Creativity and artificial intelligence</td>\n",
       "      <td>Boden, Margaret A.</td>\n",
       "      <td>August 1998</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Artificial cognition for social human–robot in...</td>\n",
       "      <td>Lemaignan, Séverin, Warnier, Mathieu and 3 more</td>\n",
       "      <td>June 2017</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Explanation in artificial intelligence: Insigh...</td>\n",
       "      <td>Miller, Tim</td>\n",
       "      <td>February 2019</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Making sense of sensory input</td>\n",
       "      <td>Evans, Richard, Hernández-Orallo, José and 3 more</td>\n",
       "      <td>April 2021</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Conflict-based search for optimal multi-agent ...</td>\n",
       "      <td>Sharon, Guni, Stern, Roni, Felner, Ariel, Stur...</td>\n",
       "      <td>February 2015</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Between MDPs and semi-MDPs: A framework for te...</td>\n",
       "      <td>Sutton, Richard S., Precup, Doina, Singh, Sati...</td>\n",
       "      <td>August 1999</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>The Hanabi challenge: A new frontier for AI re...</td>\n",
       "      <td>Bard, Nolan, Foerster, Jakob N. and 13 more</td>\n",
       "      <td>March 2020</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Evaluating XAI: A comparison of rule-based and...</td>\n",
       "      <td>van der Waa, Jasper, Nieuwburg, Elisabeth, Cre...</td>\n",
       "      <td>February 2021</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Argumentation in artificial intelligence</td>\n",
       "      <td>Bench-Capon, T.J.M., Dunne, Paul E.</td>\n",
       "      <td>October 2007</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Algorithms for computing strategies in two-pla...</td>\n",
       "      <td>Bošanský, Branislav, Lisý, Viliam and 3 more</td>\n",
       "      <td>August 2016</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Multiple object tracking: A literature review</td>\n",
       "      <td>Luo, Wenhan, Xing, Junliang and 4 more</td>\n",
       "      <td>April 2021</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Selection of relevant features and examples in...</td>\n",
       "      <td>Blum, Avrim L., Langley, Pat</td>\n",
       "      <td>December 1997</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>A survey of inverse reinforcement learning: Ch...</td>\n",
       "      <td>Arora, Saurabh, Doshi, Prashant</td>\n",
       "      <td>August 2021</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Explaining individual predictions when feature...</td>\n",
       "      <td>Aas, Kjersti, Jullum, Martin, Løland, Anders</td>\n",
       "      <td>September 2021</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>A review of possible effects of cognitive bias...</td>\n",
       "      <td>Kliegr, Tomáš, Bahník, Štěpán, Fürnkranz, Joha...</td>\n",
       "      <td>June 2021</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Integrating social power into the decision-mak...</td>\n",
       "      <td>Pereira, Gonçalo, Prada, Rui, Santos, Pedro A.</td>\n",
       "      <td>December 2016</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>“That's (not) the output I expected!” On the r...</td>\n",
       "      <td>Riveiro, Maria, Thill, Serge</td>\n",
       "      <td>September 2021</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Explaining black-box classifiers using post-ho...</td>\n",
       "      <td>Kenny, Eoin M., Ford, Courtney, Quinn, Molly, ...</td>\n",
       "      <td>May 2021</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Algorithm runtime prediction: Methods &amp; evalua...</td>\n",
       "      <td>Hutter, Frank, Xu, Lin, Hoos, Holger H., Leyto...</td>\n",
       "      <td>January 2014</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Wrappers for feature subset selection</td>\n",
       "      <td>Kohavi, Ron, John, George H.</td>\n",
       "      <td>December 1997</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Commonsense visual sensemaking for autonomous ...</td>\n",
       "      <td>Suchan, Jakob, Bhatt, Mehul, Varadarajan, Srik...</td>\n",
       "      <td>October 2021</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Paper_titles  \\\n",
       "0                                    Reward is enough   \n",
       "1                                    Reward is enough   \n",
       "2                           Making sense of raw input   \n",
       "3   Law and logic: A review from an argumentation ...   \n",
       "4              Creativity and artificial intelligence   \n",
       "5   Artificial cognition for social human–robot in...   \n",
       "6   Explanation in artificial intelligence: Insigh...   \n",
       "7                       Making sense of sensory input   \n",
       "8   Conflict-based search for optimal multi-agent ...   \n",
       "9   Between MDPs and semi-MDPs: A framework for te...   \n",
       "10  The Hanabi challenge: A new frontier for AI re...   \n",
       "11  Evaluating XAI: A comparison of rule-based and...   \n",
       "12           Argumentation in artificial intelligence   \n",
       "13  Algorithms for computing strategies in two-pla...   \n",
       "14      Multiple object tracking: A literature review   \n",
       "15  Selection of relevant features and examples in...   \n",
       "16  A survey of inverse reinforcement learning: Ch...   \n",
       "17  Explaining individual predictions when feature...   \n",
       "18  A review of possible effects of cognitive bias...   \n",
       "19  Integrating social power into the decision-mak...   \n",
       "20  “That's (not) the output I expected!” On the r...   \n",
       "21  Explaining black-box classifiers using post-ho...   \n",
       "22  Algorithm runtime prediction: Methods & evalua...   \n",
       "23              Wrappers for feature subset selection   \n",
       "24  Commonsense visual sensemaking for autonomous ...   \n",
       "\n",
       "                                        Authors_names  Published_date  \\\n",
       "0   Silver, David, Singh, Satinder, Precup, Doina,...  [October 2021]   \n",
       "1   Silver, David, Singh, Satinder, Precup, Doina,...    October 2021   \n",
       "2           Evans, Richard, Bošnjak, Matko and 5 more    October 2021   \n",
       "3                   Prakken, Henry, Sartor, Giovanni     October 2015   \n",
       "4                                 Boden, Margaret A.      August 1998   \n",
       "5     Lemaignan, Séverin, Warnier, Mathieu and 3 more       June 2017   \n",
       "6                                        Miller, Tim    February 2019   \n",
       "7   Evans, Richard, Hernández-Orallo, José and 3 more      April 2021   \n",
       "8   Sharon, Guni, Stern, Roni, Felner, Ariel, Stur...   February 2015   \n",
       "9   Sutton, Richard S., Precup, Doina, Singh, Sati...     August 1999   \n",
       "10        Bard, Nolan, Foerster, Jakob N. and 13 more      March 2020   \n",
       "11  van der Waa, Jasper, Nieuwburg, Elisabeth, Cre...   February 2021   \n",
       "12               Bench-Capon, T.J.M., Dunne, Paul E.     October 2007   \n",
       "13       Bošanský, Branislav, Lisý, Viliam and 3 more     August 2016   \n",
       "14             Luo, Wenhan, Xing, Junliang and 4 more      April 2021   \n",
       "15                      Blum, Avrim L., Langley, Pat    December 1997   \n",
       "16                   Arora, Saurabh, Doshi, Prashant      August 2021   \n",
       "17      Aas, Kjersti, Jullum, Martin, Løland, Anders   September 2021   \n",
       "18  Kliegr, Tomáš, Bahník, Štěpán, Fürnkranz, Joha...       June 2021   \n",
       "19    Pereira, Gonçalo, Prada, Rui, Santos, Pedro A.    December 2016   \n",
       "20                      Riveiro, Maria, Thill, Serge   September 2021   \n",
       "21  Kenny, Eoin M., Ford, Courtney, Quinn, Molly, ...        May 2021   \n",
       "22  Hutter, Frank, Xu, Lin, Hoos, Holger H., Leyto...    January 2014   \n",
       "23                      Kohavi, Ron, John, George H.    December 1997   \n",
       "24  Suchan, Jakob, Bhatt, Mehul, Varadarajan, Srik...    October 2021   \n",
       "\n",
       "                                            Paper_url  \n",
       "0   https://www.sciencedirect.com/science/article/...  \n",
       "1   https://www.sciencedirect.com/science/article/...  \n",
       "2   https://www.sciencedirect.com/science/article/...  \n",
       "3   https://www.sciencedirect.com/science/article/...  \n",
       "4   https://www.sciencedirect.com/science/article/...  \n",
       "5   https://www.sciencedirect.com/science/article/...  \n",
       "6   https://www.sciencedirect.com/science/article/...  \n",
       "7   https://www.sciencedirect.com/science/article/...  \n",
       "8   https://www.sciencedirect.com/science/article/...  \n",
       "9   https://www.sciencedirect.com/science/article/...  \n",
       "10  https://www.sciencedirect.com/science/article/...  \n",
       "11  https://www.sciencedirect.com/science/article/...  \n",
       "12  https://www.sciencedirect.com/science/article/...  \n",
       "13  https://www.sciencedirect.com/science/article/...  \n",
       "14  https://www.sciencedirect.com/science/article/...  \n",
       "15  https://www.sciencedirect.com/science/article/...  \n",
       "16  https://www.sciencedirect.com/science/article/...  \n",
       "17  https://www.sciencedirect.com/science/article/...  \n",
       "18  https://www.sciencedirect.com/science/article/...  \n",
       "19  https://www.sciencedirect.com/science/article/...  \n",
       "20  https://www.sciencedirect.com/science/article/...  \n",
       "21  https://www.sciencedirect.com/science/article/...  \n",
       "22  https://www.sciencedirect.com/science/article/...  \n",
       "23  https://www.sciencedirect.com/science/article/...  \n",
       "24  https://www.sciencedirect.com/science/article/...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip install bs4\n",
    "!pip install requests\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "page = requests.get(\"https://www.journals.elsevier.com/artificial-intelligence/most-downloaded-articles\")\n",
    "soup=BeautifulSoup(page.content)\n",
    "titles = []\n",
    "paper_titles = soup.find('h2', class_=\"sc-1qrq3sd-1 MKjKb sc-1nmom32-0 sc-1nmom32-1 hqhUYH ebTA-dR\")\n",
    "for i in soup.find_all('h2',class_=\"sc-1qrq3sd-1 MKjKb sc-1nmom32-0 sc-1nmom32-1 hqhUYH ebTA-dR\"):\n",
    "    paper_titles.append(i.text)\n",
    "\n",
    "authors = []\n",
    "authors_names = soup.find('span',class_=\"sc-1w3fpd7-0 pgLAT\")\n",
    "for i in soup.find_all('span',class_=\"sc-1w3fpd7-0 pgLAT\"):\n",
    "    authors_names.append(i.text)\n",
    "\n",
    "published_date =[]\n",
    "published_date = soup.find('span',class_=\"sc-1thf9ly-2 bKddwo\")\n",
    "for i in soup.find_all('span',class_=\"sc-1thf9ly-2 bKddwo\"):\n",
    "    published_date.append(i.text)\n",
    "\n",
    "paper_url = []\n",
    "\n",
    "for i in soup.find_all('a',class_=\"sc-5smygv-0 nrDZj\"):\n",
    "    paper_url.append(i['href'])\n",
    "\n",
    "data = list(zip(paper_titles,authors_names,published_date,paper_url))\n",
    "df = pd.DataFrame(data,columns=[\"Paper_titles\",\"Authors_names\",\"Published_date\",\"Paper_url\"])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be382c0e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d11136f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b5cf4ef2",
   "metadata": {},
   "source": [
    "# Q9.Python program to scrape mentioned details from dineout.co.in\n",
    "\n",
    "i) Restaurant name\n",
    "\n",
    "ii) Cuisine\n",
    "\n",
    "iii) Location\n",
    "\n",
    "iv) Ratings\n",
    "\n",
    "v) Image URL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d4528d",
   "metadata": {},
   "source": [
    "# Scrapping as follows :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "83fced25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in c:\\users\\clickmoney\\anaconda3\\lib\\site-packages (2.26.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\clickmoney\\anaconda3\\lib\\site-packages (from requests) (1.26.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\clickmoney\\anaconda3\\lib\\site-packages (from requests) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\clickmoney\\anaconda3\\lib\\site-packages (from requests) (3.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\clickmoney\\anaconda3\\lib\\site-packages (from requests) (2021.10.8)\n",
      "Requirement already satisfied: html5lib in c:\\users\\clickmoney\\anaconda3\\lib\\site-packages (1.1)\n",
      "Requirement already satisfied: six>=1.9 in c:\\users\\clickmoney\\anaconda3\\lib\\site-packages (from html5lib) (1.16.0)\n",
      "Requirement already satisfied: webencodings in c:\\users\\clickmoney\\anaconda3\\lib\\site-packages (from html5lib) (0.5.1)\n",
      "Requirement already satisfied: bs4 in c:\\users\\clickmoney\\anaconda3\\lib\\site-packages (0.0.1)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\clickmoney\\anaconda3\\lib\\site-packages (from bs4) (4.10.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\clickmoney\\anaconda3\\lib\\site-packages (from beautifulsoup4->bs4) (2.2.1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Restaurant</th>\n",
       "      <th>Location</th>\n",
       "      <th>Cuisine</th>\n",
       "      <th>Ratings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Signature Grills</td>\n",
       "      <td>Necklace Road, Secunderabad</td>\n",
       "      <td>₹ 1,500 for 2 (approx) | North Indian, Chinese...</td>\n",
       "      <td>4.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ohri's Jiva</td>\n",
       "      <td>White House,Begumpet, Secunderabad</td>\n",
       "      <td>₹ 1,200 for 2 (approx) | North Indian, Chinese...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Headquarters</td>\n",
       "      <td>Somajiguda, Central East Hyderabad</td>\n",
       "      <td>₹ 1,700 for 2 (approx) | Chinese, Continental,...</td>\n",
       "      <td>4.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Jalpaan</td>\n",
       "      <td>Oasis Centre,Begumpet, Secunderabad</td>\n",
       "      <td>₹ 1,200 for 2 (approx) | North Indian, Chinese...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Krishnapatnam</td>\n",
       "      <td>Shreshta Aura,Jubilee Hills, Central West Hyde...</td>\n",
       "      <td>₹ 1,800 for 2 (approx) | Andhra, South Indian,...</td>\n",
       "      <td>3.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>7 Sisters</td>\n",
       "      <td>Banjara Hills, Central East Hyderabad</td>\n",
       "      <td>₹ 500 for 2 (approx) | Japanese, Asian</td>\n",
       "      <td>4.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Exotica</td>\n",
       "      <td>12th Square Building,Banjara Hills, Central Ea...</td>\n",
       "      <td>₹ 1,500 for 2 (approx) | North Indian, Chinese...</td>\n",
       "      <td>4.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Amogham - The Lake View Restaurant</td>\n",
       "      <td>Khairatabad, Central Hyderabad</td>\n",
       "      <td>₹ 1,000 for 2 (approx) | Chinese, North Indian...</td>\n",
       "      <td>3.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>10 Downing Street</td>\n",
       "      <td>Lifestyle Building,Begumpet, Secunderabad</td>\n",
       "      <td>₹ 2,000 for 2 (approx) | Finger Food, Chinese,...</td>\n",
       "      <td>4.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ABs - Absolute Barbecues</td>\n",
       "      <td>Banjara Hills, Central East Hyderabad</td>\n",
       "      <td>₹ 1,400 for 2 (approx) | North Indian, Contine...</td>\n",
       "      <td>4.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Pakka Local Signature</td>\n",
       "      <td>Banjara Hills, Central East Hyderabad</td>\n",
       "      <td>₹ 1,200 for 2 (approx) | Andhra, North Indian,...</td>\n",
       "      <td>4.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Barbeque Nation</td>\n",
       "      <td>ANR Center,Banjara Hills, Central East Hyderabad</td>\n",
       "      <td>₹ 2,000 for 2 (approx) | North Indian, Chinese</td>\n",
       "      <td>4.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Ohri's Gufaa</td>\n",
       "      <td>Basheerbagh, Central Hyderabad</td>\n",
       "      <td>₹ 1,200 for 2 (approx) | North Indian, Awadhi</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>By The Bay - Bar Exchange</td>\n",
       "      <td>Khairatabad, Central Hyderabad</td>\n",
       "      <td>₹ 1,900 for 2 (approx) | North Indian, Contine...</td>\n",
       "      <td>4.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Journey 1853 - An Ode To Sahib Sindh Sultan</td>\n",
       "      <td>City Centre Mall,Banjara Hills, Central East H...</td>\n",
       "      <td>₹ 1,400 for 2 (approx) | North Indian, Mughlai</td>\n",
       "      <td>4.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Bikanervala</td>\n",
       "      <td>Banjara Hills, Central East Hyderabad</td>\n",
       "      <td>₹ 1,300 for 2 (approx) | North Indian, Street ...</td>\n",
       "      <td>4.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>A'La Liberty</td>\n",
       "      <td>Leela Gopal Towers,Banjara Hills, Central East...</td>\n",
       "      <td>₹ 1,000 for 2 (approx) | North Indian, Chinese...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Parampara - Flavours of india</td>\n",
       "      <td>ANR Center,Banjara Hills, Central East Hyderabad</td>\n",
       "      <td>₹ 1,000 for 2 (approx) | Chinese, North Indian</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Verandah</td>\n",
       "      <td>The Park,Somajiguda, Central East Hyderabad</td>\n",
       "      <td>₹ 3,300 for 2 (approx) | Chinese, North Indian...</td>\n",
       "      <td>3.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Kismet</td>\n",
       "      <td>The Park,Somajiguda, Central East Hyderabad</td>\n",
       "      <td>₹ 3,000 for 2 (approx) | Finger Food</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Up &amp; Above</td>\n",
       "      <td>SNS Arcade,PG Road, Secunderabad</td>\n",
       "      <td>₹ 1,500 for 2 (approx) | North Indian, Chinese...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     Restaurant  \\\n",
       "0                              Signature Grills   \n",
       "1                                   Ohri's Jiva   \n",
       "2                                  Headquarters   \n",
       "3                                       Jalpaan   \n",
       "4                                 Krishnapatnam   \n",
       "5                                     7 Sisters   \n",
       "6                                       Exotica   \n",
       "7            Amogham - The Lake View Restaurant   \n",
       "8                             10 Downing Street   \n",
       "9                      ABs - Absolute Barbecues   \n",
       "10                        Pakka Local Signature   \n",
       "11                              Barbeque Nation   \n",
       "12                                 Ohri's Gufaa   \n",
       "13                    By The Bay - Bar Exchange   \n",
       "14  Journey 1853 - An Ode To Sahib Sindh Sultan   \n",
       "15                                  Bikanervala   \n",
       "16                                 A'La Liberty   \n",
       "17                Parampara - Flavours of india   \n",
       "18                                     Verandah   \n",
       "19                                       Kismet   \n",
       "20                                   Up & Above   \n",
       "\n",
       "                                             Location  \\\n",
       "0                         Necklace Road, Secunderabad   \n",
       "1                  White House,Begumpet, Secunderabad   \n",
       "2                  Somajiguda, Central East Hyderabad   \n",
       "3                 Oasis Centre,Begumpet, Secunderabad   \n",
       "4   Shreshta Aura,Jubilee Hills, Central West Hyde...   \n",
       "5               Banjara Hills, Central East Hyderabad   \n",
       "6   12th Square Building,Banjara Hills, Central Ea...   \n",
       "7                      Khairatabad, Central Hyderabad   \n",
       "8           Lifestyle Building,Begumpet, Secunderabad   \n",
       "9               Banjara Hills, Central East Hyderabad   \n",
       "10              Banjara Hills, Central East Hyderabad   \n",
       "11   ANR Center,Banjara Hills, Central East Hyderabad   \n",
       "12                     Basheerbagh, Central Hyderabad   \n",
       "13                     Khairatabad, Central Hyderabad   \n",
       "14  City Centre Mall,Banjara Hills, Central East H...   \n",
       "15              Banjara Hills, Central East Hyderabad   \n",
       "16  Leela Gopal Towers,Banjara Hills, Central East...   \n",
       "17   ANR Center,Banjara Hills, Central East Hyderabad   \n",
       "18        The Park,Somajiguda, Central East Hyderabad   \n",
       "19        The Park,Somajiguda, Central East Hyderabad   \n",
       "20                   SNS Arcade,PG Road, Secunderabad   \n",
       "\n",
       "                                              Cuisine Ratings  \n",
       "0   ₹ 1,500 for 2 (approx) | North Indian, Chinese...     4.1  \n",
       "1   ₹ 1,200 for 2 (approx) | North Indian, Chinese...       4  \n",
       "2   ₹ 1,700 for 2 (approx) | Chinese, Continental,...     4.1  \n",
       "3   ₹ 1,200 for 2 (approx) | North Indian, Chinese...       4  \n",
       "4   ₹ 1,800 for 2 (approx) | Andhra, South Indian,...     3.9  \n",
       "5              ₹ 500 for 2 (approx) | Japanese, Asian     4.4  \n",
       "6   ₹ 1,500 for 2 (approx) | North Indian, Chinese...     4.4  \n",
       "7   ₹ 1,000 for 2 (approx) | Chinese, North Indian...     3.8  \n",
       "8   ₹ 2,000 for 2 (approx) | Finger Food, Chinese,...     4.3  \n",
       "9   ₹ 1,400 for 2 (approx) | North Indian, Contine...     4.3  \n",
       "10  ₹ 1,200 for 2 (approx) | Andhra, North Indian,...     4.3  \n",
       "11     ₹ 2,000 for 2 (approx) | North Indian, Chinese     4.2  \n",
       "12      ₹ 1,200 for 2 (approx) | North Indian, Awadhi       4  \n",
       "13  ₹ 1,900 for 2 (approx) | North Indian, Contine...     4.1  \n",
       "14     ₹ 1,400 for 2 (approx) | North Indian, Mughlai     4.1  \n",
       "15  ₹ 1,300 for 2 (approx) | North Indian, Street ...     4.2  \n",
       "16  ₹ 1,000 for 2 (approx) | North Indian, Chinese...       4  \n",
       "17     ₹ 1,000 for 2 (approx) | Chinese, North Indian       4  \n",
       "18  ₹ 3,300 for 2 (approx) | Chinese, North Indian...     3.9  \n",
       "19               ₹ 3,000 for 2 (approx) | Finger Food       4  \n",
       "20  ₹ 1,500 for 2 (approx) | North Indian, Chinese...       4  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip install requests\n",
    "!pip install html5lib\n",
    "!pip install bs4\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "page = requests.get(\"https://www.dineout.co.in/hyderabad-restaurants?loc=Hyderabad\")\n",
    "soup=BeautifulSoup(page.content)\n",
    "\n",
    "restaurant_name = []\n",
    "for i in soup.find_all('a', class_=\"restnt-name ellipsis\"):\n",
    "    restaurant_name.append(i.text)\n",
    "    \n",
    "location =[]\n",
    "for i in soup.find_all('div', class_=\"restnt-loc ellipsis\"):\n",
    "    location.append(i.text)    \n",
    "\n",
    "cuisine = []\n",
    "for i in soup.find_all('span', class_=\"double-line-ellipsis\"):\n",
    "    cuisine.append(i.text)\n",
    "\n",
    "ratings = []\n",
    "for i in soup.find_all('div', class_=\"restnt-rating rating-4\"):\n",
    "    ratings.append(i.text)\n",
    "    \n",
    "url = []\n",
    "for i in soup.find_all('img', class_='no-img'):\n",
    "    url.append(i['data-src'])\n",
    "    \n",
    "data = list(zip(restaurant_name,location,cuisine,ratings))\n",
    "df = pd.DataFrame(data,columns=[\"Restaurant\",\"Location\",\"Cuisine\",\"Ratings\"])\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98fb585e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01fc7422",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "45f36311",
   "metadata": {},
   "source": [
    "# Q10.Python program to scrape the details of top publications from Google Scholar from\n",
    "https://scholar.google.com/citations?view_op=top_venues&hl=en\n",
    "    \n",
    "i) Rank\n",
    "\n",
    "ii) Publication\n",
    "\n",
    "iii) h5-index\n",
    "\n",
    "iv) h5-median"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05cb8292",
   "metadata": {},
   "source": [
    "# Scrapping as follows :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "355e66c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bs4 in c:\\users\\clickmoney\\anaconda3\\lib\\site-packages (0.0.1)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\clickmoney\\anaconda3\\lib\\site-packages (from bs4) (4.10.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\clickmoney\\anaconda3\\lib\\site-packages (from beautifulsoup4->bs4) (2.2.1)\n",
      "Requirement already satisfied: requests in c:\\users\\clickmoney\\anaconda3\\lib\\site-packages (2.26.0)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\clickmoney\\anaconda3\\lib\\site-packages (from requests) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\clickmoney\\anaconda3\\lib\\site-packages (from requests) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\clickmoney\\anaconda3\\lib\\site-packages (from requests) (1.26.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\clickmoney\\anaconda3\\lib\\site-packages (from requests) (3.2)\n",
      "Requirement already satisfied: html5 in c:\\users\\clickmoney\\anaconda3\\lib\\site-packages (0.0.9)\n",
      "Requirement already satisfied: setuptools>=18.5 in c:\\users\\clickmoney\\anaconda3\\lib\\site-packages (from html5) (58.0.4)\n",
      "Requirement already satisfied: webencodings in c:\\users\\clickmoney\\anaconda3\\lib\\site-packages (from html5) (0.5.1)\n",
      "Requirement already satisfied: six in c:\\users\\clickmoney\\anaconda3\\lib\\site-packages (from html5) (1.16.0)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rank</th>\n",
       "      <th>Publication</th>\n",
       "      <th>h5_index</th>\n",
       "      <th>h5_median</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.</td>\n",
       "      <td>Nature</td>\n",
       "      <td>[444]</td>\n",
       "      <td>667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.</td>\n",
       "      <td>Nature</td>\n",
       "      <td>444</td>\n",
       "      <td>667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.</td>\n",
       "      <td>The New England Journal of Medicine</td>\n",
       "      <td>667</td>\n",
       "      <td>780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.</td>\n",
       "      <td>Science</td>\n",
       "      <td>432</td>\n",
       "      <td>614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.</td>\n",
       "      <td>IEEE/CVF Conference on Computer Vision and Pat...</td>\n",
       "      <td>780</td>\n",
       "      <td>627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>96.</td>\n",
       "      <td>Journal of Business Research</td>\n",
       "      <td>296</td>\n",
       "      <td>233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>97.</td>\n",
       "      <td>Molecular Cancer</td>\n",
       "      <td>173</td>\n",
       "      <td>209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>98.</td>\n",
       "      <td>Sensors</td>\n",
       "      <td>228</td>\n",
       "      <td>201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>99.</td>\n",
       "      <td>Nature Climate Change</td>\n",
       "      <td>173</td>\n",
       "      <td>228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>100.</td>\n",
       "      <td>IEEE Internet of Things Journal</td>\n",
       "      <td>217</td>\n",
       "      <td>212</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>101 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Rank                                        Publication h5_index  \\\n",
       "0      1.                                             Nature    [444]   \n",
       "1      1.                                             Nature      444   \n",
       "2      2.                The New England Journal of Medicine      667   \n",
       "3      3.                                            Science      432   \n",
       "4      4.  IEEE/CVF Conference on Computer Vision and Pat...      780   \n",
       "..    ...                                                ...      ...   \n",
       "96    96.                       Journal of Business Research      296   \n",
       "97    97.                                   Molecular Cancer      173   \n",
       "98    98.                                            Sensors      228   \n",
       "99    99.                              Nature Climate Change      173   \n",
       "100  100.                    IEEE Internet of Things Journal      217   \n",
       "\n",
       "    h5_median  \n",
       "0         667  \n",
       "1         667  \n",
       "2         780  \n",
       "3         614  \n",
       "4         627  \n",
       "..        ...  \n",
       "96        233  \n",
       "97        209  \n",
       "98        201  \n",
       "99        228  \n",
       "100       212  \n",
       "\n",
       "[101 rows x 4 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip install bs4\n",
    "!pip install requests\n",
    "!pip install html5\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "page = requests.get(\"https://scholar.google.com/citations?view_op=top_venues&hl=en\")\n",
    "soup=BeautifulSoup(page.content)\n",
    "Rank = []\n",
    "Rank = soup.find('td', class_=\"gsc_mvt_p\")\n",
    "for i in soup.find_all('td', class_=\"gsc_mvt_p\"):\n",
    "    Rank.append(i.text)\n",
    "    \n",
    "Publication = []\n",
    "Publication = soup.find('td', class_=\"gsc_mvt_t\")\n",
    "for i in soup.find_all('td', class_=\"gsc_mvt_t\"):\n",
    "    Publication.append(i.text)\n",
    "    \n",
    "h5_index=[]\n",
    "h5_index= soup.find('td', class_=\"gsc_mvt_n\")\n",
    "for i in soup.find_all('td', class_=\"gsc_mvt_n\"):\n",
    "    h5_index.append(i.text)\n",
    "    \n",
    "h5_median =[]\n",
    "h5_median = soup.find('span', class_=\"gs_ibl gsc_mp_anchor\")\n",
    "for i in soup.find_all('span', class_=\"gs_ibl gsc_mp_anchor\"):\n",
    "    h5_median.append(i.text)\n",
    "\n",
    "data = list(zip(Rank,Publication,h5_index,h5_median))\n",
    "df = pd.DataFrame(data,columns=[\"Rank\",\"Publication\",\"h5_index\",\"h5_median\"])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46f694bc",
   "metadata": {},
   "source": [
    "# --------------------------END OF WEBSCRAPPING ASSIGNMENT -1--------------------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
