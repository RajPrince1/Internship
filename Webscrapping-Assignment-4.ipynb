{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe0bf940",
   "metadata": {},
   "source": [
    "# Q-1.Scrape the details of most viewed videos on YouTube from Wikipedia.\n",
    "Url = https://en.wikipedia.org/wiki/List_of_most-viewed_YouTube_videos\n",
    "You need to find following details:\n",
    "A) Rank\n",
    "B) Name\n",
    "C) Artist\n",
    "D) Upload date\n",
    "E) Views"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f0448f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing the necessary libraries\n",
    "\n",
    "import selenium\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import ElementClickInterceptedException\n",
    "from selenium.common.exceptions import StaleElementReferenceException, NoSuchElementException\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d678ad14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Artist</th>\n",
       "      <th>Uploaded Date</th>\n",
       "      <th>Views</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Baby Shark Dance</td>\n",
       "      <td>Pinkfong Baby Shark - Kids' Songs &amp; Stories</td>\n",
       "      <td>June 17, 2016</td>\n",
       "      <td>7,046,700,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Despacito</td>\n",
       "      <td>Luis Fonsi</td>\n",
       "      <td>January 12, 2017</td>\n",
       "      <td>3,002,800,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>See You Again</td>\n",
       "      <td>Wiz Khalifa</td>\n",
       "      <td>April 6, 2015</td>\n",
       "      <td>2,894,000,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Gangnam Style</td>\n",
       "      <td>Psy</td>\n",
       "      <td>July 15, 2012</td>\n",
       "      <td>803,700,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Baby</td>\n",
       "      <td>Justin Bieber</td>\n",
       "      <td>February 19, 2010</td>\n",
       "      <td>245,400,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Bad Romance</td>\n",
       "      <td>Lady Gaga</td>\n",
       "      <td>November 24, 2009</td>\n",
       "      <td>178,400,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Charlie Bit My Finger</td>\n",
       "      <td>HDCYT</td>\n",
       "      <td>May 22, 2007</td>\n",
       "      <td>128,900,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Evolution of Dance</td>\n",
       "      <td>Judson Laipply</td>\n",
       "      <td>April 6, 2006</td>\n",
       "      <td>118,900,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Girlfriend</td>\n",
       "      <td>RCA Records</td>\n",
       "      <td>February 27, 2007</td>\n",
       "      <td>92,600,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Evolution of Dance</td>\n",
       "      <td>Judson Laipply</td>\n",
       "      <td>April 6, 2006</td>\n",
       "      <td>78,400,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Music Is My Hot Hot Sex</td>\n",
       "      <td>CLARUSBARTEL72</td>\n",
       "      <td>April 9, 2007</td>\n",
       "      <td>76,600,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Evolution of Dance</td>\n",
       "      <td>Judson Laipply</td>\n",
       "      <td>April 6, 2006</td>\n",
       "      <td>10,600,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Pokemon Theme Music Video</td>\n",
       "      <td>Smosh</td>\n",
       "      <td>November 28, 2005</td>\n",
       "      <td>4,300,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Myspace – The Movie</td>\n",
       "      <td>eggtea</td>\n",
       "      <td>January 31, 2006</td>\n",
       "      <td>2,700,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Phony Photo Booth</td>\n",
       "      <td>mugenized</td>\n",
       "      <td>December 1, 2005</td>\n",
       "      <td>3,400,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>The Chronic of Narnia Rap</td>\n",
       "      <td>youtubedude</td>\n",
       "      <td>December 18, 2005</td>\n",
       "      <td>2,300,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Ronaldinho: Touch of Gold</td>\n",
       "      <td>Nikesoccer</td>\n",
       "      <td>October 21, 2005</td>\n",
       "      <td>255,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>I/O Brush</td>\n",
       "      <td>larfus</td>\n",
       "      <td>October 5, 2005</td>\n",
       "      <td>247,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Me at the zoo</td>\n",
       "      <td>jawed</td>\n",
       "      <td>April 23, 2005</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         Name                                       Artist  \\\n",
       "0            Baby Shark Dance  Pinkfong Baby Shark - Kids' Songs & Stories   \n",
       "1                   Despacito                                   Luis Fonsi   \n",
       "2               See You Again                                  Wiz Khalifa   \n",
       "3               Gangnam Style                                          Psy   \n",
       "4                        Baby                                Justin Bieber   \n",
       "5                 Bad Romance                                    Lady Gaga   \n",
       "6       Charlie Bit My Finger                                        HDCYT   \n",
       "7          Evolution of Dance                               Judson Laipply   \n",
       "8                  Girlfriend                                  RCA Records   \n",
       "9          Evolution of Dance                               Judson Laipply   \n",
       "10    Music Is My Hot Hot Sex                               CLARUSBARTEL72   \n",
       "11         Evolution of Dance                               Judson Laipply   \n",
       "12  Pokemon Theme Music Video                                        Smosh   \n",
       "13        Myspace – The Movie                                       eggtea   \n",
       "14          Phony Photo Booth                                    mugenized   \n",
       "15  The Chronic of Narnia Rap                                  youtubedude   \n",
       "16  Ronaldinho: Touch of Gold                                   Nikesoccer   \n",
       "17                  I/O Brush                                       larfus   \n",
       "18              Me at the zoo                                        jawed   \n",
       "\n",
       "        Uploaded Date          Views  \n",
       "0       June 17, 2016  7,046,700,000  \n",
       "1    January 12, 2017  3,002,800,000  \n",
       "2       April 6, 2015  2,894,000,000  \n",
       "3       July 15, 2012    803,700,000  \n",
       "4   February 19, 2010    245,400,000  \n",
       "5   November 24, 2009    178,400,000  \n",
       "6        May 22, 2007    128,900,000  \n",
       "7       April 6, 2006    118,900,000  \n",
       "8   February 27, 2007     92,600,000  \n",
       "9       April 6, 2006     78,400,000  \n",
       "10      April 9, 2007     76,600,000  \n",
       "11      April 6, 2006     10,600,000  \n",
       "12  November 28, 2005      4,300,000  \n",
       "13   January 31, 2006      2,700,000  \n",
       "14   December 1, 2005      3,400,000  \n",
       "15  December 18, 2005      2,300,000  \n",
       "16   October 21, 2005        255,000  \n",
       "17    October 5, 2005        247,000  \n",
       "18     April 23, 2005              1  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Lets first connect ro webdriver\n",
    "driver=webdriver.Chrome(r\"chromedriver.exe\")\n",
    "#Opening target url page on automated chrome browser\n",
    "driver.get(\"https://en.wikipedia.org/wiki/List_of_most-viewed_YouTube_videos\")\n",
    "driver.maximize_window()\n",
    "\n",
    "time.sleep(2)\n",
    "\n",
    "#creating the empty list\n",
    "Name=[]\n",
    "Artist=[]\n",
    "Upload=[]\n",
    "Views=[]\n",
    "\n",
    "#scraping Song Name\n",
    "for i in driver.find_elements(By.XPATH,\"//*[@id='mw-content-text']/div[1]/table[3]/tbody/tr/td[1]\"):\n",
    "    Name.append(i.text.split('\"')[1])\n",
    "\n",
    "#scraping Artist name\n",
    "for i in driver.find_elements(By.XPATH,\"//*[@id='mw-content-text']/div[1]/table[3]/tbody/tr/td[2]\"):\n",
    "    Artist.append(i.text.split('[')[0])\n",
    "\n",
    "#scraping the No of Views\n",
    "for i in driver.find_elements(By.XPATH,\"//*[@id='mw-content-text']/div[1]/table[3]/tbody/tr/td[3]\"):\n",
    "    Views.append(i.text)\n",
    "\n",
    "#scraping the Uploaded date\n",
    "for i in driver.find_elements(By.XPATH,\"//*[@id='mw-content-text']/div[1]/table[3]/tbody/tr/td[4]\"):\n",
    "    Upload.append(i.text)\n",
    "    \n",
    "Yotube={'Name':Name,'Artist':Artist,'Uploaded Date':Upload,'Views':Views}\n",
    "df=pd.DataFrame(data=Yotube)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3dc4b66",
   "metadata": {},
   "source": [
    "# Q-2.Scrape the details team India’s international fixtures from bcci.tv. Url = https://www.bcci.tv\n",
    "You need to find following details:\n",
    "A) Match title (I.e. 1st ODI)\n",
    "B) Series\n",
    "C) Place\n",
    "D) Date\n",
    "E) Time\n",
    "Note: - From bcci.tv home page you have reach to the international fixture page through code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f90614d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Team India International Fixtures\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Series</th>\n",
       "      <th>Place</th>\n",
       "      <th>Date</th>\n",
       "      <th>Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SOUTH AFRICA TOUR OF INDIA ODI SERIES 2022-23</td>\n",
       "      <td>India vs South Africa</td>\n",
       "      <td>Bharat Ratna Shri Atal Bihari Vajpayee Ekana C...</td>\n",
       "      <td>6 OCT 2022</td>\n",
       "      <td>1:30 PM IST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ASIA CUP WOMENS 2022</td>\n",
       "      <td>India Women vs Pakistan Women</td>\n",
       "      <td>Sylhet International Cricket Stadium,</td>\n",
       "      <td>7 OCT 2022</td>\n",
       "      <td>1:00 PM IST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ASIA CUP WOMENS 2022</td>\n",
       "      <td>India Women vs Bangladesh Women</td>\n",
       "      <td>Sylhet International Cricket Stadium,</td>\n",
       "      <td>8 OCT 2022</td>\n",
       "      <td>1:00 PM IST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SOUTH AFRICA TOUR OF INDIA ODI SERIES 2022-23</td>\n",
       "      <td>India vs South Africa</td>\n",
       "      <td>JSCA International Stadium Complex,</td>\n",
       "      <td>9 OCT 2022</td>\n",
       "      <td>1:30 PM IST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ASIA CUP WOMENS 2022</td>\n",
       "      <td>India Women vs Thailand Women</td>\n",
       "      <td>Sylhet International Cricket Stadium,</td>\n",
       "      <td>10 OCT 2022</td>\n",
       "      <td>1:00 PM IST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>SOUTH AFRICA TOUR OF INDIA ODI SERIES 2022-23</td>\n",
       "      <td>India vs South Africa</td>\n",
       "      <td>Arun Jaitley Stadium,</td>\n",
       "      <td>11 OCT 2022</td>\n",
       "      <td>1:30 PM IST</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Title  \\\n",
       "0  SOUTH AFRICA TOUR OF INDIA ODI SERIES 2022-23   \n",
       "1                           ASIA CUP WOMENS 2022   \n",
       "2                           ASIA CUP WOMENS 2022   \n",
       "3  SOUTH AFRICA TOUR OF INDIA ODI SERIES 2022-23   \n",
       "4                           ASIA CUP WOMENS 2022   \n",
       "5  SOUTH AFRICA TOUR OF INDIA ODI SERIES 2022-23   \n",
       "\n",
       "                            Series  \\\n",
       "0            India vs South Africa   \n",
       "1    India Women vs Pakistan Women   \n",
       "2  India Women vs Bangladesh Women   \n",
       "3            India vs South Africa   \n",
       "4    India Women vs Thailand Women   \n",
       "5            India vs South Africa   \n",
       "\n",
       "                                               Place         Date         Time  \n",
       "0  Bharat Ratna Shri Atal Bihari Vajpayee Ekana C...   6 OCT 2022  1:30 PM IST  \n",
       "1              Sylhet International Cricket Stadium,   7 OCT 2022  1:00 PM IST  \n",
       "2              Sylhet International Cricket Stadium,   8 OCT 2022  1:00 PM IST  \n",
       "3                JSCA International Stadium Complex,   9 OCT 2022  1:30 PM IST  \n",
       "4              Sylhet International Cricket Stadium,  10 OCT 2022  1:00 PM IST  \n",
       "5                              Arun Jaitley Stadium,  11 OCT 2022  1:30 PM IST  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Lets first connect ro webdriver\n",
    "driver=webdriver.Chrome(r\"chromedriver.exe\")\n",
    "#Opening target url page on automated chrome browser\n",
    "driver.get(\"https://www.bcci.tv\")\n",
    "driver.maximize_window()\n",
    "\n",
    "time.sleep(5)\n",
    "\n",
    "#clicking on International tab\n",
    "driver.find_element(By.XPATH,'//*[@id=\"navigation\"]/ul[1]/li[2]/a').click()\n",
    "\n",
    "time.sleep(1)\n",
    "#creating the empty list\n",
    "Title=[]\n",
    "Series=[]\n",
    "Place=[]\n",
    "Date=[]\n",
    "Time=[]\n",
    "\n",
    "#scraping the Title Name\n",
    "for i in driver.find_elements(By.XPATH,'//span[@class=\"ng-binding\"]'):\n",
    "    Title.append(i.text)\n",
    "\n",
    "#scraping the series name\n",
    "for i in driver.find_elements(By.XPATH,'//div[@class=\"fixture-card-mid d-flex align-items-center justify-content-between\"]'):\n",
    "    Series.append(i.text.replace('\\n',' '))\n",
    "\n",
    "#scraping the place\n",
    "for i in driver.find_elements(By.XPATH,'//span[@class=\"ng-binding ng-scope\"]'):\n",
    "    Place.append(i.text)\n",
    "    \n",
    "#scraping the date\n",
    "for i in driver.find_elements(By.XPATH,'//h5[@class=\"ng-binding\"]'):\n",
    "    Date.append(i.text)\n",
    "\n",
    "#scraping the time\n",
    "for i in driver.find_elements(By.XPATH,'//h5[@class=\"text-right ng-binding\"]'):\n",
    "    Time.append(i.text)\n",
    "    \n",
    "#creating the DataFrame\n",
    "bcci={'Title':Title,'Series':Series,'Place':Place,'Date':Date,'Time':Time}\n",
    "df=pd.DataFrame(data=bcci)\n",
    "print('Team India International Fixtures')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "896dbef7",
   "metadata": {},
   "source": [
    "# Q-3.Scrape the details of selenium exception from guru99.com.                     Url = https://www.guru99.com\n",
    "You need to find following details:\n",
    "\n",
    "A) Name\n",
    "\n",
    "B) Description\n",
    "\n",
    "Note: - From guru99 home page you have to reach to selenium exception handling page through code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bca42ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets first connect ro webdriver\n",
    "driver=webdriver.Chrome(r\"chromedriver.exe\")\n",
    "#Opening target url page on automated chrome browser\n",
    "driver.get(\"https://www.guru99.com/\")\n",
    "driver.maximize_window()\n",
    "\n",
    "time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f5208fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#clicking on selenium tutorial\n",
    "driver.find_element(By.XPATH,\"//*[@id='java_technologies']/li[3]/a\").click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f5c5d78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#clicking on Selenium exceptions\n",
    "driver.find_element(By.XPATH,\"/html/body/div[1]/div/div/div/main/div/article/div/div/table[5]/tbody/tr[34]/td[1]/a\").click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7aa03126",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of Selenium Exceptions\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1. ElementNotVisibleException</td>\n",
       "      <td>This type of Selenium exception occurs when a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            Name  \\\n",
       "0  1. ElementNotVisibleException   \n",
       "\n",
       "                                         Description  \n",
       "0   This type of Selenium exception occurs when a...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#creating the empty list\n",
    "Name=[]\n",
    "Description=[]\n",
    "\n",
    "#scraping for name   \n",
    "for i in driver.find_elements(By.XPATH,'/html/body/div[1]/div/div/div/main/div/article/div/div/p[1]'):\n",
    "    Name.append(i.text.split(':')[0])\n",
    "    \n",
    "#scraping the description\n",
    "for i in driver.find_elements(By.XPATH,'/html/body/div[1]/div/div/div/main/div/article/div/div/p[1]'):\n",
    "    Description.append(i.text.split(':')[1])\n",
    "    \n",
    "time.sleep(1)\n",
    "#creating the dataframe\n",
    "selenium={'Name':Name,'Description':Description}\n",
    "df=pd.DataFrame(data=selenium)\n",
    "print('List of Selenium Exceptions')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "523f5942",
   "metadata": {},
   "source": [
    " !! theres no class name so couldnot scrapped all exceptions "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a0dc573",
   "metadata": {},
   "source": [
    "# Q-4.Scrape the details of State-wise GDP of India from statisticstime.com.      Url = http://statisticstimes.com\n",
    "\n",
    "You have to find following details:\n",
    "\n",
    "A) Rank\n",
    "\n",
    "B) State\n",
    "\n",
    "C) GSDP(18-19)\n",
    "\n",
    "D) GSDP(17-18)\n",
    "\n",
    "E) Share(2017)\n",
    "\n",
    "F) GDP($ billion)\n",
    "\n",
    "Note: - From statisticstimes home page you have to reach to economy page through code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f49772fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets first connect ro webdriver\n",
    "driver=webdriver.Chrome(r\"chromedriver.exe\")\n",
    "#maximising the window\n",
    "driver.maximize_window()\n",
    "#Opening target url page on automated chrome browser\n",
    "driver.get(\"http://statisticstimes.com/\")\n",
    "\n",
    "\n",
    "time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8d23847a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State-wise GDP of India from www.statisticstime.com\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rank</th>\n",
       "      <th>State</th>\n",
       "      <th>GSDP 19-20</th>\n",
       "      <th>GSDP 18-19</th>\n",
       "      <th>Share 18-19</th>\n",
       "      <th>GDP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Maharashtra</td>\n",
       "      <td>-</td>\n",
       "      <td>2,632,792</td>\n",
       "      <td>13.94%</td>\n",
       "      <td>399.921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Tamil Nadu</td>\n",
       "      <td>1,845,853</td>\n",
       "      <td>1,630,208</td>\n",
       "      <td>8.63%</td>\n",
       "      <td>247.629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Uttar Pradesh</td>\n",
       "      <td>1,687,818</td>\n",
       "      <td>1,584,764</td>\n",
       "      <td>8.39%</td>\n",
       "      <td>240.726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Gujarat</td>\n",
       "      <td>-</td>\n",
       "      <td>1,502,899</td>\n",
       "      <td>7.96%</td>\n",
       "      <td>228.290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Karnataka</td>\n",
       "      <td>1,631,977</td>\n",
       "      <td>1,493,127</td>\n",
       "      <td>7.91%</td>\n",
       "      <td>226.806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>West Bengal</td>\n",
       "      <td>1,253,832</td>\n",
       "      <td>1,089,898</td>\n",
       "      <td>5.77%</td>\n",
       "      <td>165.556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>Rajasthan</td>\n",
       "      <td>1,020,989</td>\n",
       "      <td>942,586</td>\n",
       "      <td>4.99%</td>\n",
       "      <td>143.179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>Andhra Pradesh</td>\n",
       "      <td>972,782</td>\n",
       "      <td>862,957</td>\n",
       "      <td>4.57%</td>\n",
       "      <td>131.083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>Telangana</td>\n",
       "      <td>969,604</td>\n",
       "      <td>861,031</td>\n",
       "      <td>4.56%</td>\n",
       "      <td>130.791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>Madhya Pradesh</td>\n",
       "      <td>906,672</td>\n",
       "      <td>809,592</td>\n",
       "      <td>4.29%</td>\n",
       "      <td>122.977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>Kerala</td>\n",
       "      <td>-</td>\n",
       "      <td>781,653</td>\n",
       "      <td>4.14%</td>\n",
       "      <td>118.733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>Delhi</td>\n",
       "      <td>856,112</td>\n",
       "      <td>774,870</td>\n",
       "      <td>4.10%</td>\n",
       "      <td>117.703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>Haryana</td>\n",
       "      <td>831,610</td>\n",
       "      <td>734,163</td>\n",
       "      <td>3.89%</td>\n",
       "      <td>111.519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>Bihar</td>\n",
       "      <td>611,804</td>\n",
       "      <td>530,363</td>\n",
       "      <td>2.81%</td>\n",
       "      <td>80.562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>Punjab</td>\n",
       "      <td>574,760</td>\n",
       "      <td>526,376</td>\n",
       "      <td>2.79%</td>\n",
       "      <td>79.957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>Odisha</td>\n",
       "      <td>521,275</td>\n",
       "      <td>487,805</td>\n",
       "      <td>2.58%</td>\n",
       "      <td>74.098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>Assam</td>\n",
       "      <td>-</td>\n",
       "      <td>315,881</td>\n",
       "      <td>1.67%</td>\n",
       "      <td>47.982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>Chhattisgarh</td>\n",
       "      <td>329,180</td>\n",
       "      <td>304,063</td>\n",
       "      <td>1.61%</td>\n",
       "      <td>46.187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>Jharkhand</td>\n",
       "      <td>328,598</td>\n",
       "      <td>297,204</td>\n",
       "      <td>1.57%</td>\n",
       "      <td>45.145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>Uttarakhand</td>\n",
       "      <td>-</td>\n",
       "      <td>245,895</td>\n",
       "      <td>1.30%</td>\n",
       "      <td>37.351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>21</td>\n",
       "      <td>Jammu &amp; Kashmir</td>\n",
       "      <td>-</td>\n",
       "      <td>155,956</td>\n",
       "      <td>0.83%</td>\n",
       "      <td>23.690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>22</td>\n",
       "      <td>Himachal Pradesh</td>\n",
       "      <td>165,472</td>\n",
       "      <td>153,845</td>\n",
       "      <td>0.81%</td>\n",
       "      <td>23.369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>23</td>\n",
       "      <td>Goa</td>\n",
       "      <td>80,449</td>\n",
       "      <td>73,170</td>\n",
       "      <td>0.39%</td>\n",
       "      <td>11.115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>24</td>\n",
       "      <td>Tripura</td>\n",
       "      <td>55,984</td>\n",
       "      <td>49,845</td>\n",
       "      <td>0.26%</td>\n",
       "      <td>7.571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>25</td>\n",
       "      <td>Chandigarh</td>\n",
       "      <td>-</td>\n",
       "      <td>42,114</td>\n",
       "      <td>0.22%</td>\n",
       "      <td>6.397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>26</td>\n",
       "      <td>Puducherry</td>\n",
       "      <td>38,253</td>\n",
       "      <td>34,433</td>\n",
       "      <td>0.18%</td>\n",
       "      <td>5.230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>27</td>\n",
       "      <td>Meghalaya</td>\n",
       "      <td>36,572</td>\n",
       "      <td>33,481</td>\n",
       "      <td>0.18%</td>\n",
       "      <td>5.086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>28</td>\n",
       "      <td>Sikkim</td>\n",
       "      <td>32,496</td>\n",
       "      <td>28,723</td>\n",
       "      <td>0.15%</td>\n",
       "      <td>4.363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>29</td>\n",
       "      <td>Manipur</td>\n",
       "      <td>31,790</td>\n",
       "      <td>27,870</td>\n",
       "      <td>0.15%</td>\n",
       "      <td>4.233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>30</td>\n",
       "      <td>Nagaland</td>\n",
       "      <td>-</td>\n",
       "      <td>27,283</td>\n",
       "      <td>0.14%</td>\n",
       "      <td>4.144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>31</td>\n",
       "      <td>Arunachal Pradesh</td>\n",
       "      <td>-</td>\n",
       "      <td>24,603</td>\n",
       "      <td>0.13%</td>\n",
       "      <td>3.737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>32</td>\n",
       "      <td>Mizoram</td>\n",
       "      <td>26,503</td>\n",
       "      <td>22,287</td>\n",
       "      <td>0.12%</td>\n",
       "      <td>3.385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>33</td>\n",
       "      <td>Andaman &amp; Nicobar Islands</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Rank                      State GSDP 19-20 GSDP 18-19 Share 18-19      GDP\n",
       "0     1                Maharashtra          -  2,632,792      13.94%  399.921\n",
       "1     2                 Tamil Nadu  1,845,853  1,630,208       8.63%  247.629\n",
       "2     3              Uttar Pradesh  1,687,818  1,584,764       8.39%  240.726\n",
       "3     4                    Gujarat          -  1,502,899       7.96%  228.290\n",
       "4     5                  Karnataka  1,631,977  1,493,127       7.91%  226.806\n",
       "5     6                West Bengal  1,253,832  1,089,898       5.77%  165.556\n",
       "6     7                  Rajasthan  1,020,989    942,586       4.99%  143.179\n",
       "7     8             Andhra Pradesh    972,782    862,957       4.57%  131.083\n",
       "8     9                  Telangana    969,604    861,031       4.56%  130.791\n",
       "9    10             Madhya Pradesh    906,672    809,592       4.29%  122.977\n",
       "10   11                     Kerala          -    781,653       4.14%  118.733\n",
       "11   12                      Delhi    856,112    774,870       4.10%  117.703\n",
       "12   13                    Haryana    831,610    734,163       3.89%  111.519\n",
       "13   14                      Bihar    611,804    530,363       2.81%   80.562\n",
       "14   15                     Punjab    574,760    526,376       2.79%   79.957\n",
       "15   16                     Odisha    521,275    487,805       2.58%   74.098\n",
       "16   17                      Assam          -    315,881       1.67%   47.982\n",
       "17   18               Chhattisgarh    329,180    304,063       1.61%   46.187\n",
       "18   19                  Jharkhand    328,598    297,204       1.57%   45.145\n",
       "19   20                Uttarakhand          -    245,895       1.30%   37.351\n",
       "20   21            Jammu & Kashmir          -    155,956       0.83%   23.690\n",
       "21   22           Himachal Pradesh    165,472    153,845       0.81%   23.369\n",
       "22   23                        Goa     80,449     73,170       0.39%   11.115\n",
       "23   24                    Tripura     55,984     49,845       0.26%    7.571\n",
       "24   25                 Chandigarh          -     42,114       0.22%    6.397\n",
       "25   26                 Puducherry     38,253     34,433       0.18%    5.230\n",
       "26   27                  Meghalaya     36,572     33,481       0.18%    5.086\n",
       "27   28                     Sikkim     32,496     28,723       0.15%    4.363\n",
       "28   29                    Manipur     31,790     27,870       0.15%    4.233\n",
       "29   30                   Nagaland          -     27,283       0.14%    4.144\n",
       "30   31          Arunachal Pradesh          -     24,603       0.13%    3.737\n",
       "31   32                    Mizoram     26,503     22,287       0.12%    3.385\n",
       "32   33  Andaman & Nicobar Islands          -          -           -        -"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#getting to the Indian page under economy tab\n",
    "page=driver.find_element(By.XPATH,\"//div[@class='dropdown-content']/a[3]\")\n",
    "driver.get(page.get_attribute('href'))\n",
    "\n",
    "#clicking on GDP of indian states\n",
    "driver.find_element(By.XPATH,\"//ul[@style='list-style-type:none;margin-left:20px;']/li[1]/a\").click()\n",
    "\n",
    "#creating the empty list\n",
    "Rank=[]\n",
    "state=[]\n",
    "GSDP1=[]\n",
    "GSDP2=[]\n",
    "share=[]\n",
    "GDP=[]\n",
    "\n",
    "#scraping for name\n",
    "for i in driver.find_elements(By.XPATH,\"//*[@id='table_id']/tbody/tr/td[1]\"):\n",
    "    Rank.append(i.text)\n",
    "\n",
    "#scraping for state name\n",
    "for i in driver.find_elements(By.XPATH,\"//*[@id='table_id']/tbody/tr/td[2]\"):\n",
    "    state.append(i.text)\n",
    "\n",
    "#scraping for GSDP1\n",
    "for i in driver.find_elements(By.XPATH,\"//*[@id='table_id']/tbody/tr/td[3]\"):\n",
    "    GSDP1.append(i.text)\n",
    "\n",
    "#scraping for GSDP1\n",
    "for i in driver.find_elements(By.XPATH,\"//*[@id='table_id']/tbody/tr/td[4]\"):\n",
    "    GSDP2.append(i.text)\n",
    "    \n",
    "#scraping for share\n",
    "for i in driver.find_elements(By.XPATH,\"//*[@id='table_id']/tbody/tr/td[5]\"):\n",
    "    share.append(i.text)\n",
    "    \n",
    "#scraping for GDP\n",
    "for i in driver.find_elements(By.XPATH,\"//*[@id='table_id']/tbody/tr/td[6]\"):\n",
    "    GDP.append(i.text)\n",
    "    \n",
    "time.sleep(1)\n",
    "#creating the DataFrame\n",
    "gdp_State={'Rank':Rank,'State':state,'GSDP 19-20':GSDP1,'GSDP 18-19':GSDP2,'Share 18-19':share,'GDP':GDP}\n",
    "df=pd.DataFrame(data=gdp_State)\n",
    "print ('State-wise GDP of India from www.statisticstime.com')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0024a0f5",
   "metadata": {},
   "source": [
    "# Q-5.Scrape the details of trending repositories on Github.com.                      Url = https://github.com/\n",
    "You have to find the following details:\n",
    "\n",
    "A) Repository title\n",
    "\n",
    "B) Repository description\n",
    "\n",
    "C) Contributors count\n",
    "\n",
    "D) Language used\n",
    "\n",
    "Note: - From the home page you have to click on the trending option from Explore menu through code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f5cdd37e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading the drivers and url\n",
    "driver=webdriver.Chrome('chromedriver.exe')\n",
    "driver.get('https://github.com/')\n",
    "driver.maximize_window()\n",
    "\n",
    "#click opensource button\n",
    "driver.find_element(By.XPATH,'/html/body/div[1]/header/div/div[2]/div/nav/ul/li[3]/button').click()\n",
    "\n",
    "#click trending\n",
    "driver.find_element(By.XPATH,'/html/body/div[1]/header/div/div[2]/div/nav/ul/li[3]/div/ul[3]/li[3]/a').click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "55adfc39",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "All arrays must be of the same length",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\CLICKM~1\\AppData\\Local\\Temp/ipykernel_23108/3077987699.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[1;31m# saving the lists as dataframe\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 50\u001b[1;33m \u001b[0mdf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m\"Name\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mrep_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"Description\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mrep_description\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"Contributor\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mrep_count\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"Language\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mrep_language\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     51\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Trending repositories on Github'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[0;32m    612\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    613\u001b[0m             \u001b[1;31m# GH#38939 de facto copy defaults to False only in non-dict cases\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 614\u001b[1;33m             \u001b[0mmgr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdict_to_mgr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtyp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmanager\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    615\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMaskedArray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    616\u001b[0m             \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmrecords\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mmrecords\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\internals\\construction.py\u001b[0m in \u001b[0;36mdict_to_mgr\u001b[1;34m(data, index, columns, dtype, typ, copy)\u001b[0m\n\u001b[0;32m    462\u001b[0m         \u001b[1;31m# TODO: can we get rid of the dt64tz special case above?\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    463\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 464\u001b[1;33m     return arrays_to_mgr(\n\u001b[0m\u001b[0;32m    465\u001b[0m         \u001b[0marrays\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_names\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtyp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtyp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconsolidate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    466\u001b[0m     )\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\internals\\construction.py\u001b[0m in \u001b[0;36marrays_to_mgr\u001b[1;34m(arrays, arr_names, index, columns, dtype, verify_integrity, typ, consolidate)\u001b[0m\n\u001b[0;32m    117\u001b[0m         \u001b[1;31m# figure out the index, if necessary\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    118\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mindex\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 119\u001b[1;33m             \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_extract_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    120\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    121\u001b[0m             \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mensure_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\internals\\construction.py\u001b[0m in \u001b[0;36m_extract_index\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m    633\u001b[0m             \u001b[0mlengths\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_lengths\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    634\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 635\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"All arrays must be of the same length\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    636\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    637\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhave_dicts\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: All arrays must be of the same length"
     ]
    }
   ],
   "source": [
    "url_list=[]\n",
    "url=driver.find_elements(By.XPATH,\"//h1[@class='h3 lh-condensed']/a\")\n",
    "\n",
    "for i in url[:]:\n",
    "    url_list.append(i.get_attribute('href'))\n",
    "time.sleep(10)\n",
    "#creating empty list\n",
    "name=[]\n",
    "url=[]\n",
    "description=[]\n",
    "count=[]\n",
    "language=[]\n",
    "\n",
    "for i in url_list:\n",
    "    driver.get(i)\n",
    "# scraping repositry name\n",
    "rep_name = driver.find_elements(By.XPATH,'//strong[@class=\"mr-2 flex-self-stretch\"]')# xpath for brand\n",
    "for i in rep_name:\n",
    "        name.append(i.text) # saving in the list \n",
    "\n",
    "        time.sleep(1)\n",
    "# scraping description\n",
    "rep_description = driver.find_elements(By.XPATH,'//p[@class=\"f4 my-3\"]') # xpath for description\n",
    "for i in rep_description:\n",
    "        try:\n",
    "            description.append(i.text)# saving in the list\n",
    "        except:\n",
    "            description.append(\"_\")\n",
    "            \n",
    "            time.sleep(1)\n",
    "# scraping contribution count\n",
    "rep_count = driver.find_elements(By.XPATH,'/html/body/div[4]/div/main/turbo-frame/div/div/div/div[3]/div[2]/div/div[5]/div/h2/a/span')# xpath for price\n",
    "for i in rep_count:\n",
    "        try:\n",
    "            count.append(i.text)# saving in the list\n",
    "        except:\n",
    "            count.append(\"_\")\n",
    "            \n",
    "            time.sleep(1)\n",
    "# scraping Language\n",
    "rep_language = driver.find_elements(By.XPATH,\"/html/body/div[4]/div/main/turbo-frame/div/div/div/div[3]/div[2]/div/div[6]/div/ul\")# xpath for price\n",
    "for i in rep_count:\n",
    "        try:\n",
    "            language.append(i.text)# saving in the list\n",
    "        except:\n",
    "            landuage.append(\"_\")\n",
    "time.sleep(5)\n",
    "\n",
    "# saving the lists as dataframe\n",
    "df=pd.DataFrame({\"Name\":rep_name,\"Description\":rep_description,\"Contributor\":rep_count,\"Language\":rep_language})\n",
    "print('Trending repositories on Github')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21d684a3",
   "metadata": {},
   "source": [
    "# Q.6-Scrape the details of top 100 songs on billiboard.com.                               Url = https:/www.billboard.com\n",
    "You have to find the following details:\n",
    "\n",
    "A) Song name\n",
    "\n",
    "B) Artist name\n",
    "C) Last week rank\n",
    "\n",
    "D) Peak rank\n",
    "\n",
    "E) Weeks on board\n",
    "\n",
    "Note: - From the home page you have to click on the charts option then hot 100-page link through code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eea19f5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 100 songs on Billboard\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Song Name</th>\n",
       "      <th>Artist Name</th>\n",
       "      <th>Last Week Rank</th>\n",
       "      <th>Peak Rank</th>\n",
       "      <th>Weeks on board</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Song Name, Artist Name, Last Week Rank, Peak Rank, Weeks on board]\n",
       "Index: []"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Lets first connect ro webdriver\n",
    "driver=webdriver.Chrome(r\"chromedriver.exe\")\n",
    "#Opening target url page on automated chrome browser\n",
    "driver.get(\"https:/www.billboard.com/\")\n",
    "driver.maximize_window()\n",
    "\n",
    "time.sleep(2)\n",
    "#getting the Hot 100 page\n",
    "hot100=driver.find_element(By.XPATH,'//*[@id=\"main-wrapper\"]/header/div/div[3]/div/nav/ul/li[1]/a')\n",
    "hot100.click()\n",
    "\n",
    "time.sleep(2)\n",
    "#creating the empty list\n",
    "song=[]\n",
    "name=[]\n",
    "lastweek=[]\n",
    "peak=[]\n",
    "week=[]\n",
    "time.sleep(1)\n",
    "\n",
    "#scraping for song name\n",
    "for i in driver.find_elements(By.XPATH,'/html/body/div[3]/main/div[2]/div[3]/div/div/div/div[2]/div[2]/ul/li[4]/ul/li[1]/h3'):\n",
    "    song.append(i.text)\n",
    "\n",
    "#scraping for name of the artist\n",
    "for i in driver.find_elements(By.XPATH,'/html/body/div[3]/main/div[2]/div[3]/div/div/div/div[2]/div[3]/ul/li[4]/ul/li[1]/span'):\n",
    "    name.append(i.text)\n",
    "    \n",
    "#scraping for last week\n",
    "for i in driver.find_elements(By.XPATH,'/html/body/div[3]/main/div[2]/div[3]/div/div/div/div[2]/div[3]/ul/li[4]/ul/li[4]/span'):\n",
    "    lastweek.append(i.text)\n",
    "    \n",
    "#scraping for peak rank\n",
    "for i in driver.find_elements(By.XPATH,'/html/body/div[3]/main/div[2]/div[3]/div/div/div/div[2]/div[2]/ul/li[4]/ul/li[5]/span'):\n",
    "    peak.append(i.text)\n",
    "    \n",
    "#scraping for Weeks on board\n",
    "for i in driver.find_elements(By.XPATH,'/html/body/div[3]/main/div[2]/div[3]/div/div/div/div[2]/div[2]/ul/li[4]/ul/li[6]/span'):\n",
    "    week.append(i.text)\n",
    "    \n",
    "#creating the dataframe\n",
    "Billboard={'Song Name':song,'Artist Name':name,'Last Week Rank':lastweek,'Peak Rank':peak,'Weeks on board':week}\n",
    "df=pd.DataFrame(data=Billboard)\n",
    "print('Top 100 songs on Billboard')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bcb701e",
   "metadata": {},
   "source": [
    "# Q.7-Scrape the details of Data science recruiters from naukri.com.                  Url = https://www.naukri.com/\n",
    "You have to find the following details:\n",
    "\n",
    "A) Name\n",
    "\n",
    "B) Designation\n",
    "\n",
    "C) Company\n",
    "\n",
    "D) Skills they hire for\n",
    "\n",
    "E) Location\n",
    "\n",
    "Note: - From naukri.com homepage click on the recruiters option and the on the search pane type Data science and click on search. All this should be done through code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5064cbdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Science recruiters from Naukri\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Recruiter Name</th>\n",
       "      <th>Designation</th>\n",
       "      <th>Company Name</th>\n",
       "      <th>Location</th>\n",
       "      <th>Skills they Hire for</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Aakash Harit</td>\n",
       "      <td>HR Manager</td>\n",
       "      <td>Data Science Network</td>\n",
       "      <td>Data Science Network</td>\n",
       "      <td>Classic ASP Developer , Internet Marketing Pro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Recruiter Name Designation          Company Name              Location  \\\n",
       "0   Aakash Harit  HR Manager  Data Science Network  Data Science Network   \n",
       "\n",
       "                                Skills they Hire for  \n",
       "0  Classic ASP Developer , Internet Marketing Pro...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Lets first connect ro webdriver\n",
    "driver=webdriver.Chrome(r\"chromedriver.exe\")\n",
    "driver.get('https://www.naukri.com/data-science-recruiters')\n",
    "time.sleep(3)\n",
    "\n",
    "\n",
    "url_list=[]\n",
    "url=driver.find_elements(By.XPATH,'/html/body/div[3]/div/div[2]/div/div[2]/div[1]/div[1]/div[1]/div[1]/p/a[1]')\n",
    "\n",
    "for i in url[:2]:\n",
    "    url_list.append(i.get_attribute('href'))\n",
    "time.sleep(20)\n",
    "\n",
    "name=[]\n",
    "designation=[]\n",
    "company=[]\n",
    "skills=[]\n",
    "location=[]\n",
    "\n",
    "for i in url_list:\n",
    "    driver.get(i)\n",
    "# scraping recruiter name\n",
    "rec_name = driver.find_elements(By.XPATH,'/html/body/div[2]/div[1]/div[1]/div[2]/div[2]/h1')# xpath for brand\n",
    "for i in rec_name:\n",
    "    name.append(i.text) # saving in the list \n",
    "\n",
    "#scrapping recruiter designation\n",
    "rec_designation = driver.find_elements(By.XPATH,'/html/body/div[2]/div[1]/div[1]/div[2]/div[3]')# xpath for brand\n",
    "for i in rec_designation:\n",
    "    designation.append(i.text) # saving in the list \n",
    "\n",
    "#scrapping company name\n",
    "company_name = driver.find_elements(By.XPATH,'/html/body/div[2]/div[1]/div[1]/div[2]/div[4]')# xpath for brand\n",
    "for i in company_name:\n",
    "    company.append(i.text) # saving in the list \n",
    "        \n",
    "#scraping the recruiter location\n",
    "    try:\n",
    "        rec_location = driver.find_elements(By.XPATH,'/html/body/div[2]/div[1]/div[1]/div[2]/div[5]') # xpath for description\n",
    "        location.append(i.text)# saving in the list\n",
    "    except NoSuchElementException:\n",
    "        location.append(\"-\")\n",
    "\n",
    "#scraping the price of the sunglasses\n",
    "rec_skills = driver.find_elements(By.XPATH,'/html/body/div[2]/div[1]/div[2]/div[2]/div[1]/p[1]')# xpath for price\n",
    "for i in rec_skills:\n",
    "        skills.append(i.text)# saving in the list\n",
    "time.sleep(5)\n",
    "\n",
    "# saving the lists as dataframe\n",
    "df=pd.DataFrame({\"Recruiter Name\":name[:50],\"Designation\":designation[:50],\"Company Name\":company[:50],\"Location\":location[:50],\"Skills they Hire for\":skills[:50]})\n",
    "print('Data Science recruiters from Naukri')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e3a234e",
   "metadata": {},
   "source": [
    "# Q-8.Scrape the details of Highest selling novels.\n",
    "Url = https://www.theguardian.com/news/datablog/2012/aug/09/best-selling-books-all-time-fifty-shades-grey- compare/\n",
    "You have to find the following details:\n",
    "\n",
    "A) Book name\n",
    "\n",
    "B) Author name\n",
    "\n",
    "C) Volumes sold\n",
    "\n",
    "D) Publisher\n",
    "\n",
    "E) Genre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a2ce36be",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets first connect ro webdriver\n",
    "driver=webdriver.Chrome(r\"chromedriver.exe\")\n",
    "#Opening target url page on automated chrome browser\n",
    "driver.get(\"https://www.theguardian.com/news/datablog/2012/aug/09/best-selling-books-all-time-fifty-shades-grey-compare\")\n",
    "driver.maximize_window()\n",
    "\n",
    "time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2f16e9b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Highest selling Novels\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Book Name</th>\n",
       "      <th>Author Name</th>\n",
       "      <th>Volumes sold</th>\n",
       "      <th>Publisher</th>\n",
       "      <th>Genre</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Da Vinci Code,The</td>\n",
       "      <td>Brown, Dan</td>\n",
       "      <td>5,094,805</td>\n",
       "      <td>Transworld</td>\n",
       "      <td>Crime, Thriller &amp; Adventure</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Harry Potter and the Deathly Hallows</td>\n",
       "      <td>Rowling, J.K.</td>\n",
       "      <td>4,475,152</td>\n",
       "      <td>Bloomsbury</td>\n",
       "      <td>Children's Fiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Harry Potter and the Philosopher's Stone</td>\n",
       "      <td>Rowling, J.K.</td>\n",
       "      <td>4,200,654</td>\n",
       "      <td>Bloomsbury</td>\n",
       "      <td>Children's Fiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Harry Potter and the Order of the Phoenix</td>\n",
       "      <td>Rowling, J.K.</td>\n",
       "      <td>4,179,479</td>\n",
       "      <td>Bloomsbury</td>\n",
       "      <td>Children's Fiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Fifty Shades of Grey</td>\n",
       "      <td>James, E. L.</td>\n",
       "      <td>3,758,936</td>\n",
       "      <td>Random House</td>\n",
       "      <td>Romance &amp; Sagas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>Ghost,The</td>\n",
       "      <td>Harris, Robert</td>\n",
       "      <td>807,311</td>\n",
       "      <td>Random House</td>\n",
       "      <td>General &amp; Literary Fiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>Happy Days with the Naked Chef</td>\n",
       "      <td>Oliver, Jamie</td>\n",
       "      <td>794,201</td>\n",
       "      <td>Penguin</td>\n",
       "      <td>Food &amp; Drink: General</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>Hunger Games,The:Hunger Games Trilogy</td>\n",
       "      <td>Collins, Suzanne</td>\n",
       "      <td>792,187</td>\n",
       "      <td>Scholastic Ltd.</td>\n",
       "      <td>Young Adult Fiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>Lost Boy,The:A Foster Child's Search for the L...</td>\n",
       "      <td>Pelzer, Dave</td>\n",
       "      <td>791,507</td>\n",
       "      <td>Orion</td>\n",
       "      <td>Biography: General</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>Jamie's Ministry of Food:Anyone Can Learn to C...</td>\n",
       "      <td>Oliver, Jamie</td>\n",
       "      <td>791,095</td>\n",
       "      <td>Penguin</td>\n",
       "      <td>Food &amp; Drink: General</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Book Name       Author Name  \\\n",
       "0                                   Da Vinci Code,The        Brown, Dan   \n",
       "1                Harry Potter and the Deathly Hallows     Rowling, J.K.   \n",
       "2            Harry Potter and the Philosopher's Stone     Rowling, J.K.   \n",
       "3           Harry Potter and the Order of the Phoenix     Rowling, J.K.   \n",
       "4                                Fifty Shades of Grey      James, E. L.   \n",
       "..                                                ...               ...   \n",
       "95                                          Ghost,The    Harris, Robert   \n",
       "96                     Happy Days with the Naked Chef     Oliver, Jamie   \n",
       "97              Hunger Games,The:Hunger Games Trilogy  Collins, Suzanne   \n",
       "98  Lost Boy,The:A Foster Child's Search for the L...      Pelzer, Dave   \n",
       "99  Jamie's Ministry of Food:Anyone Can Learn to C...     Oliver, Jamie   \n",
       "\n",
       "   Volumes sold        Publisher                        Genre  \n",
       "0     5,094,805       Transworld  Crime, Thriller & Adventure  \n",
       "1     4,475,152       Bloomsbury           Children's Fiction  \n",
       "2     4,200,654       Bloomsbury           Children's Fiction  \n",
       "3     4,179,479       Bloomsbury           Children's Fiction  \n",
       "4     3,758,936     Random House              Romance & Sagas  \n",
       "..          ...              ...                          ...  \n",
       "95      807,311     Random House   General & Literary Fiction  \n",
       "96      794,201          Penguin        Food & Drink: General  \n",
       "97      792,187  Scholastic Ltd.          Young Adult Fiction  \n",
       "98      791,507            Orion           Biography: General  \n",
       "99      791,095          Penguin        Food & Drink: General  \n",
       "\n",
       "[100 rows x 5 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#creating the empty list\n",
    "name=[]\n",
    "author=[]\n",
    "volume=[]\n",
    "publisher=[]\n",
    "genre=[]\n",
    "\n",
    "#scraping for name\n",
    "for i in driver.find_elements(By.XPATH,\"//table[@class='in-article sortable']/tbody/tr/td[2]\"):\n",
    "    name.append(i.text)\n",
    "\n",
    "#scraping for author\n",
    "for i in driver.find_elements(By.XPATH,\"//table[@class='in-article sortable']/tbody/tr/td[3]\"):\n",
    "    author.append(i.text)\n",
    "\n",
    "#scraping for volume\n",
    "for i in driver.find_elements(By.XPATH,\"//table[@class='in-article sortable']/tbody/tr/td[4]\"):\n",
    "    volume.append(i.text)\n",
    "\n",
    "#scraping for publisher\n",
    "for i in driver.find_elements(By.XPATH,\"//table[@class='in-article sortable']/tbody/tr/td[5]\"):\n",
    "    publisher.append(i.text)\n",
    "\n",
    "#scraping for genre\n",
    "for i in driver.find_elements(By.XPATH,\"//table[@class='in-article sortable']/tbody/tr/td[6]\"):\n",
    "    genre.append(i.text)\n",
    "    \n",
    "#creating the Dataframe\n",
    "novels={'Book Name':name,'Author Name':author,'Volumes sold':volume,'Publisher':publisher,'Genre':genre}\n",
    "df=pd.DataFrame(data=novels)\n",
    "print('Highest selling Novels')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62295489",
   "metadata": {},
   "source": [
    "# Q-9.Scrape the details most watched tv series of all time from imdb.com.            Url = https://www.imdb.com/list/ls095964455/\n",
    "You have to find the following details:\n",
    "\n",
    "A) Name\n",
    "\n",
    "B) Year span\n",
    "\n",
    "C) Genre\n",
    "\n",
    "D) Run time\n",
    "\n",
    "E) Ratings\n",
    "\n",
    "F) Votes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "146ba2af",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets first connect ro webdriver\n",
    "driver=webdriver.Chrome(r\"chromedriver.exe\")\n",
    "#Opening target url page on automated chrome browser\n",
    "driver.get(\"https://www.imdb.com/list/ls095964455/\")\n",
    "driver.maximize_window()\n",
    "\n",
    "time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3e05c28b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most watched TV Series of all time from IMDB.COM\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Year Span</th>\n",
       "      <th>Genre</th>\n",
       "      <th>Run time</th>\n",
       "      <th>Ratings</th>\n",
       "      <th>Votes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Game of Thrones</td>\n",
       "      <td>(2011–2019)</td>\n",
       "      <td>Action, Adventure, Drama</td>\n",
       "      <td>57 min</td>\n",
       "      <td>9.2</td>\n",
       "      <td>2,061,758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Stranger Things</td>\n",
       "      <td>(2016– )</td>\n",
       "      <td>Drama, Fantasy, Horror</td>\n",
       "      <td>51 min</td>\n",
       "      <td>8.7</td>\n",
       "      <td>1,155,251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The Walking Dead</td>\n",
       "      <td>(2010–2022)</td>\n",
       "      <td>Drama, Horror, Thriller</td>\n",
       "      <td>44 min</td>\n",
       "      <td>8.1</td>\n",
       "      <td>972,494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13 Reasons Why</td>\n",
       "      <td>(2017–2020)</td>\n",
       "      <td>Drama, Mystery, Thriller</td>\n",
       "      <td>60 min</td>\n",
       "      <td>7.5</td>\n",
       "      <td>289,935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The 100</td>\n",
       "      <td>(2014–2020)</td>\n",
       "      <td>Drama, Mystery, Sci-Fi</td>\n",
       "      <td>43 min</td>\n",
       "      <td>7.6</td>\n",
       "      <td>249,354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>Reign</td>\n",
       "      <td>(2013–2017)</td>\n",
       "      <td>Drama</td>\n",
       "      <td>42 min</td>\n",
       "      <td>7.4</td>\n",
       "      <td>49,695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>A Series of Unfortunate Events</td>\n",
       "      <td>(2017–2019)</td>\n",
       "      <td>Adventure, Comedy, Drama</td>\n",
       "      <td>50 min</td>\n",
       "      <td>7.8</td>\n",
       "      <td>60,752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>Criminal Minds</td>\n",
       "      <td>(2005–2020)</td>\n",
       "      <td>Crime, Drama, Mystery</td>\n",
       "      <td>42 min</td>\n",
       "      <td>8.1</td>\n",
       "      <td>195,544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>Scream: The TV Series</td>\n",
       "      <td>(2015–2019)</td>\n",
       "      <td>Comedy, Crime, Drama</td>\n",
       "      <td>45 min</td>\n",
       "      <td>7.1</td>\n",
       "      <td>41,156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>The Haunting of Hill House</td>\n",
       "      <td>(2018)</td>\n",
       "      <td>Drama, Horror, Mystery</td>\n",
       "      <td>572 min</td>\n",
       "      <td>8.6</td>\n",
       "      <td>238,198</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                              Name    Year Span                     Genre  \\\n",
       "0                  Game of Thrones  (2011–2019)  Action, Adventure, Drama   \n",
       "1                  Stranger Things     (2016– )    Drama, Fantasy, Horror   \n",
       "2                 The Walking Dead  (2010–2022)   Drama, Horror, Thriller   \n",
       "3                   13 Reasons Why  (2017–2020)  Drama, Mystery, Thriller   \n",
       "4                          The 100  (2014–2020)    Drama, Mystery, Sci-Fi   \n",
       "..                             ...          ...                       ...   \n",
       "95                           Reign  (2013–2017)                     Drama   \n",
       "96  A Series of Unfortunate Events  (2017–2019)  Adventure, Comedy, Drama   \n",
       "97                  Criminal Minds  (2005–2020)     Crime, Drama, Mystery   \n",
       "98           Scream: The TV Series  (2015–2019)      Comedy, Crime, Drama   \n",
       "99      The Haunting of Hill House       (2018)    Drama, Horror, Mystery   \n",
       "\n",
       "   Run time Ratings      Votes  \n",
       "0    57 min     9.2  2,061,758  \n",
       "1    51 min     8.7  1,155,251  \n",
       "2    44 min     8.1    972,494  \n",
       "3    60 min     7.5    289,935  \n",
       "4    43 min     7.6    249,354  \n",
       "..      ...     ...        ...  \n",
       "95   42 min     7.4     49,695  \n",
       "96   50 min     7.8     60,752  \n",
       "97   42 min     8.1    195,544  \n",
       "98   45 min     7.1     41,156  \n",
       "99  572 min     8.6    238,198  \n",
       "\n",
       "[100 rows x 6 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#creating the empty list\n",
    "name=[]\n",
    "year=[]\n",
    "genre=[]\n",
    "runtime=[]\n",
    "rating=[]\n",
    "votes=[]\n",
    "\n",
    "#scraping name of the series\n",
    "for i in driver.find_elements(By.XPATH,\"//h3[@class='lister-item-header']/a\"):\n",
    "    name.append(i.text)\n",
    "#scraping year    \n",
    "for i in driver.find_elements(By.XPATH,\"//span[@class='lister-item-year text-muted unbold']\"):\n",
    "    year.append(i.text)\n",
    "#scraping genre\n",
    "for i in driver.find_elements(By.XPATH,\"//span[@class='genre']\"):\n",
    "    genre.append(i.text)\n",
    "#scrapping runtime    \n",
    "for i in driver.find_elements(By.XPATH,\"//span[@class='runtime']\"):\n",
    "    runtime.append(i.text)\n",
    "#scraping rating\n",
    "for i in driver.find_elements(By.XPATH,\"//*[@id='main']/div/div[3]/div[3]/div/div[2]/div[1]/div[1]/span[2]\"):\n",
    "    rating.append(i.text)\n",
    "#scraping votes\n",
    "for i in driver.find_elements(By.XPATH,\"//span[@name='nv']\"):\n",
    "    votes.append(i.text)\n",
    "\n",
    "#creating the dataframe\n",
    "imdb={'Name':name,'Year Span':year,'Genre':genre,'Run time':runtime,'Ratings':rating,'Votes':votes}\n",
    "df=pd.DataFrame(data=imdb)\n",
    "print ('Most watched TV Series of all time from IMDB.COM')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfeec182",
   "metadata": {},
   "source": [
    "# Q-10.Details of Datasets from UCI machine learning repositories.                  Url = https://archive.ics.uci.edu/\n",
    "You have to find the following details:\n",
    "\n",
    "A) Dataset name\n",
    "\n",
    "B) Data type\n",
    "\n",
    "C) Task\n",
    "\n",
    "D) Attribute type\n",
    "\n",
    "E) No of instances\n",
    "\n",
    "F) No of attribute\n",
    "\n",
    "G) Year\n",
    "\n",
    "Note: - from the home page you have to go to the ShowAllDataset page through code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "605a1bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets first connect ro webdriver\n",
    "driver=webdriver.Chrome(r\"chromedriver.exe\")\n",
    "#Opening target url page on automated chrome browser\n",
    "driver.get('https://archive.ics.uci.edu/')\n",
    "driver.maximize_window()\n",
    "\n",
    "time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "70187b83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Details of datasets from UCI Machine Learning repositories\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dataset Name</th>\n",
       "      <th>Data Type</th>\n",
       "      <th>Task</th>\n",
       "      <th>Attribute Type</th>\n",
       "      <th>No of Instances</th>\n",
       "      <th>No of Attribue</th>\n",
       "      <th>Year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Abalone</td>\n",
       "      <td>Multivariate</td>\n",
       "      <td>Classification</td>\n",
       "      <td>Categorical, Integer, Real</td>\n",
       "      <td>4177</td>\n",
       "      <td>8</td>\n",
       "      <td>1995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Adult</td>\n",
       "      <td>Multivariate</td>\n",
       "      <td>Classification</td>\n",
       "      <td>Categorical, Integer</td>\n",
       "      <td>48842</td>\n",
       "      <td>14</td>\n",
       "      <td>1996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Annealing</td>\n",
       "      <td>Multivariate</td>\n",
       "      <td>Classification</td>\n",
       "      <td>Categorical, Integer, Real</td>\n",
       "      <td>798</td>\n",
       "      <td>38</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Anonymous Microsoft Web Data</td>\n",
       "      <td></td>\n",
       "      <td>Recommender-Systems</td>\n",
       "      <td>Categorical</td>\n",
       "      <td>37711</td>\n",
       "      <td>294</td>\n",
       "      <td>1998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Arrhythmia</td>\n",
       "      <td>Multivariate</td>\n",
       "      <td>Classification</td>\n",
       "      <td>Categorical, Integer, Real</td>\n",
       "      <td>452</td>\n",
       "      <td>279</td>\n",
       "      <td>1998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>617</th>\n",
       "      <td>Influenza outbreak event prediction via Twitte...</td>\n",
       "      <td>Multivariate</td>\n",
       "      <td>Classification</td>\n",
       "      <td>Integer, Real</td>\n",
       "      <td>75840</td>\n",
       "      <td>525</td>\n",
       "      <td>2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>618</th>\n",
       "      <td>Turkish Music Emotion Dataset</td>\n",
       "      <td>Multivariate</td>\n",
       "      <td>Classification</td>\n",
       "      <td>Integer, Real</td>\n",
       "      <td>400</td>\n",
       "      <td>50</td>\n",
       "      <td>2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>619</th>\n",
       "      <td>Maternal Health Risk Data Set</td>\n",
       "      <td></td>\n",
       "      <td>Classification</td>\n",
       "      <td></td>\n",
       "      <td>1014</td>\n",
       "      <td>7</td>\n",
       "      <td>2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>620</th>\n",
       "      <td>Room Occupancy Estimation</td>\n",
       "      <td>Multivariate, Time-Series</td>\n",
       "      <td>Classification</td>\n",
       "      <td>Real</td>\n",
       "      <td>10129</td>\n",
       "      <td>16</td>\n",
       "      <td>2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>621</th>\n",
       "      <td>Image Recognition Task Execution Times in Mobi...</td>\n",
       "      <td>Univariate</td>\n",
       "      <td>Regression</td>\n",
       "      <td>Real</td>\n",
       "      <td>4000</td>\n",
       "      <td>2</td>\n",
       "      <td>2021</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>622 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          Dataset Name  \\\n",
       "0                                              Abalone   \n",
       "1                                                Adult   \n",
       "2                                            Annealing   \n",
       "3                         Anonymous Microsoft Web Data   \n",
       "4                                           Arrhythmia   \n",
       "..                                                 ...   \n",
       "617  Influenza outbreak event prediction via Twitte...   \n",
       "618                      Turkish Music Emotion Dataset   \n",
       "619                      Maternal Health Risk Data Set   \n",
       "620                          Room Occupancy Estimation   \n",
       "621  Image Recognition Task Execution Times in Mobi...   \n",
       "\n",
       "                      Data Type                  Task  \\\n",
       "0                 Multivariate        Classification    \n",
       "1                 Multivariate        Classification    \n",
       "2                 Multivariate        Classification    \n",
       "3                                Recommender-Systems    \n",
       "4                 Multivariate        Classification    \n",
       "..                          ...                   ...   \n",
       "617               Multivariate        Classification    \n",
       "618               Multivariate        Classification    \n",
       "619                                   Classification    \n",
       "620  Multivariate, Time-Series        Classification    \n",
       "621                 Univariate            Regression    \n",
       "\n",
       "                  Attribute Type No of Instances No of Attribue   Year  \n",
       "0    Categorical, Integer, Real            4177              8   1995   \n",
       "1          Categorical, Integer           48842             14   1996   \n",
       "2    Categorical, Integer, Real             798             38          \n",
       "3                   Categorical           37711            294   1998   \n",
       "4    Categorical, Integer, Real             452            279   1998   \n",
       "..                           ...             ...            ...    ...  \n",
       "617               Integer, Real           75840            525   2020   \n",
       "618               Integer, Real             400             50   2020   \n",
       "619                                        1014              7   2020   \n",
       "620                        Real           10129             16   2021   \n",
       "621                        Real            4000              2   2021   \n",
       "\n",
       "[622 rows x 7 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#clicking on vew all dataset\n",
    "dataset=driver.find_element(By.XPATH,\"/html/body/table[1]/tbody/tr/td[2]/span[2]/a\")\n",
    "driver.get(dataset.get_attribute('href'))\n",
    "\n",
    "#creating the empty list\n",
    "name=[]\n",
    "datatype=[]\n",
    "task=[]\n",
    "attribute=[]\n",
    "instances=[]\n",
    "no_attribute=[]\n",
    "year=[]\n",
    "\n",
    "time.sleep(2)\n",
    "#scraping dataset name\n",
    "for i in driver.find_elements(By.XPATH,\"//tr['@bgcolor=#003366']/td/table/tbody/tr/td[2]/p/b/a\"):\n",
    "    name.append(i.text)\n",
    "#scraping datatype\n",
    "for i in driver.find_elements(By.XPATH,\"//table[@border='1']/tbody/tr/td[2]/p\"):\n",
    "    datatype.append(i.text)\n",
    "datatype=datatype[1:]\n",
    "#scraping task\n",
    "for i in driver.find_elements(By.XPATH,\"//table[@border='1']/tbody/tr/td[3]/p\"):\n",
    "    task.append(i.text)\n",
    "task=task[1:]\n",
    "#scraping attribute\n",
    "for i in driver.find_elements(By.XPATH,\"//table[@border='1']/tbody/tr/td[4]/p\"):\n",
    "    attribute.append(i.text)\n",
    "attribute=attribute[1:]\n",
    "#scraping instances\n",
    "for i in driver.find_elements(By.XPATH,\"//table[@border='1']/tbody/tr/td[5]/p\"):\n",
    "    instances.append(i.text)\n",
    "instances=instances[1:]\n",
    "#scraping attributes\n",
    "for i in driver.find_elements(By.XPATH,\"//table[@border='1']/tbody/tr/td[6]/p\"):\n",
    "    no_attribute.append(i.text)\n",
    "no_attribute=no_attribute[1:]\n",
    "#scraping year\n",
    "for i in driver.find_elements(By.XPATH,\"//table[@border='1']/tbody/tr/td[7]/p\"):\n",
    "    year.append(i.text)\n",
    "year=year[1:]\n",
    "\n",
    "#creating the dataset\n",
    "uci={'Dataset Name':name,'Data Type':datatype,'Task':task,'Attribute Type':attribute,\n",
    "    'No of Instances':instances,'No of Attribue':no_attribute,'Year':year}\n",
    "df=pd.DataFrame(data=uci)\n",
    "print('Details of datasets from UCI Machine Learning repositories')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e8cd99dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
